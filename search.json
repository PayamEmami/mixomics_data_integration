[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nMatrix Decomposition for Data Integration lab comes in two flavors, light and detailed version. See a short description on what to except from each and feel free to choose the one you want to work with during the workshop.\n\nLight version\n\nShows how we use mixOmics package for unsupervised dimensionality reduction, PCA (one data set) and GCCA (multiple data sets).\nShows how we use mixOmics package DIABLO method for supervised data integration.\nGives minimalist explanation of methods, skipping mathematical foundations. Maybe be good as first read or as a quick guide how to use mixOmics when already knowing the foundations behind the methods.\n\nTake me to the light version\n\n\nDetailed version\n\nIntroduces unsupervised GCCA method starting from basics by successively building upon PCA method and CCA. Gives mathematical foundations, R code from scratch as well as demonstrates how things can be run via mixOmics package.\nBuilds further upon GCCA to introduce PLS and PLS-DA and their explanations to supervised integration. R code from scratch and via mixOmics.\nMay take some time to go through and be slightly challenging. Recommended when wanting to understand the methods.\n\nTake me to the detailed version"
  },
  {
    "objectID": "mixomics.html",
    "href": "mixomics.html",
    "title": "Matrix Decomposition for Data Integration",
    "section": "",
    "text": "Set up the environment\n# list of packages to be installed\npackages &lt;- c(\"mixOmics\")\n\n# check and install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages, dependencies = TRUE, type = \"binary\")\n\n# load the libraries\nlibrary(mixOmics)"
  },
  {
    "objectID": "mixomics.html#data",
    "href": "mixomics.html#data",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Data",
    "text": "Data\nOur data has to be in a data.frame where features are in the columns and samples in the rows. For now we are going to use TCGA dataset from mixOmics.\n\nThis data set is a small subset of the full data set from The Cancer Genome Atlas that can be analysed with the DIABLO framework. It contains the expression or abundance of three matching omics data sets: mRNA, miRNA and proteomics for 150 breast cancer samples (with three molecular subtypes of breast cancer: Basal, Her2, Luminal A) in the training set, and 70 samples in the test set. The test set is missing the proteomics data set.\n\n\n# download the dataset\ndownload.file(\"https://github.com/mixOmicsTeam/mixOmics/raw/master/data/breast.TCGA.rda\", destfile = \"TCGA.rda\")\n\n# load the data\nload(\"TCGA.rda\")\n\nThis data has already been split into a list with two elements: training and testing. The first element (training) contains four elements, again lists, containing miRNA, mRNA, proteomics and cancer molecular subtypes. The second element (testing) contains three lists holding miRNA, mRNA and molecular subtypes data (proteomics data are missing here).\n\n# preview data\nstr(breast.TCGA)\n\nList of 2\n $ data.train:List of 4\n  ..$ mirna  : num [1:150, 1:184] 11.8 12.9 12.3 12 13.4 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"A0FJ\" \"A13E\" \"A0G0\" \"A0SX\" ...\n  .. .. ..$ : chr [1:184] \"hsa-let-7a-1\" \"hsa-let-7a-2\" \"hsa-let-7a-3\" \"hsa-let-7b\" ...\n  ..$ mrna   : num [1:150, 1:200] 4.36 1.98 1.73 4.36 2.45 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"A0FJ\" \"A13E\" \"A0G0\" \"A0SX\" ...\n  .. .. ..$ : chr [1:200] \"RTN2\" \"NDRG2\" \"CCDC113\" \"FAM63A\" ...\n  ..$ protein: num [1:150, 1:142] 0.0491 -0.08 -0.0328 -0.2053 0.0602 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"A0FJ\" \"A13E\" \"A0G0\" \"A0SX\" ...\n  .. .. ..$ : chr [1:142] \"14-3-3_epsilon\" \"4E-BP1\" \"4E-BP1_pS65\" \"4E-BP1_pT37\" ...\n  ..$ subtype: Factor w/ 3 levels \"Basal\",\"Her2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ data.test :List of 3\n  ..$ mirna  : num [1:70, 1:184] 12.8 13.9 12.9 12.4 13.1 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:70] \"A54N\" \"A2NL\" \"A6VY\" \"A3XT\" ...\n  .. .. ..$ : chr [1:184] \"hsa-let-7a-1\" \"hsa-let-7a-2\" \"hsa-let-7a-3\" \"hsa-let-7b\" ...\n  ..$ mrna   : num [1:70, 1:200] 1.19 2.73 3.05 2.7 3.14 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:70] \"A54N\" \"A2NL\" \"A6VY\" \"A3XT\" ...\n  .. .. ..$ : chr [1:200] \"RTN2\" \"NDRG2\" \"CCDC113\" \"FAM63A\" ...\n  ..$ subtype: Factor w/ 3 levels \"Basal\",\"Her2\",..: 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "mixomics.html#pca-in-r",
    "href": "mixomics.html#pca-in-r",
    "title": "Matrix Decomposition for Data Integration",
    "section": "PCA in R",
    "text": "PCA in R\nTo run PCA in R using SVD we center our data and use the svd function.\n\n# center the data\ndata_centered_mirna &lt;- scale(breast.TCGA$data.train$mirna, center = TRUE,scale = FALSE)\n\n# do SVD\nsvd_mirna &lt;- svd(data_centered_mirna)\n\n# calculate the PC scores\ncalculated_scores &lt;- data_centered_mirna%*%svd_mirna$v\n\n# plot the PC scores\nplot(calculated_scores[,1:2],xlab=\"PC1\",ylab=\"PC2\",col=breast.TCGA$data.train$subtype)\n\n\n\n\n\n\n\n\nThis gives us identical results as the ones obtained via using standard prcomp function.\n\n# do PCA using prcomp\npca_prcomp &lt;- prcomp(data_centered_mirna)\n\n# plot the PCA\nplot(pca_prcomp$x[,1:2], xlab=\"PC1\", ylab=\"PC2\", col=breast.TCGA$data.train$subtype)"
  },
  {
    "objectID": "mixomics.html#pca-with-mixomics",
    "href": "mixomics.html#pca-with-mixomics",
    "title": "Matrix Decomposition for Data Integration",
    "section": "PCA with mixOmics",
    "text": "PCA with mixOmics\nIn practice there are more specialized packages that can be used to do PCA. mixOmics implements a very powerful PCA method that provide us not only with standard PCA but also with extra advantages such as handling of missing value, plotting and taking into account repeated measurements.\n\n# do PCA using mixOmics\npca_mixomics &lt;- mixOmics::pca(data_centered_mirna, ncomp = 2)\n\n# plot the PCA\nmixOmics::plotIndiv(pca_mixomics, \n                    group=breast.TCGA$data.train$subtype, \n                    ind.names = F, \n                    legend = T, \n                    title = \"miRNA PCA\")\n\n\n\n\n\n\n\n\nIn the function above, we have performed a PCA with two components on miRNA data. The first principal component (PC1) captures 23% of the total variance, while the second principal component (PC2) accounts for an additional 9%. This means that together, PC1 and PC2 provide a simplified representation that encapsulates 32% of the total variability in the miRNA data.\nExamining the PCA plot, we can notice distinct patterns. The Basal cancer subtype is clustered on the right side of the plot, indicating an unique miRNA expression profile that is markedly different from the other subtypes. In contrast, the HER2 and LumA subtypes are more centered and somewhat intermingled towards the left, suggesting overlapping or similar patterns of miRNA expression. Most of the differences are represented in the PC1, so it is probably our most important factor to focus on!\nThis observed separation and overlap in the PCA plot is not just a graphical representation but is rooted in the underlying biology of these cancer subtypes. The positioning of the different groups on the PCA plot is influenced by the expression levels of various miRNAs, each contributing differently to the principal components.\nNow, as we go deeper into understanding the PCA plot, it becomes essential to explore the concept of loadings. Loadings help us to interpret the contribution of each miRNA to the principal components. They provide insights into which specific miRNAs are driving the separation between different cancer subtypes observed on the PCA plot.\nWe can go ahead and plot the loadings. We start with our most important PC, that is PC1\n\n# loadings for component 1\nmixOmics::plotLoadings(pca_mixomics,comp = 1)\n\n\n\n\n\n\n\n\nIn this bar plot, each bar represents a specific miRNA. The length of the bar corresponds to the value of the loading of that miRNA on PC1, indicating its contribution to this principal component. The miRNAs with the highest absolute contributions are at the bottom, and those with the lowest are at the top, making it easy to identify the most influential miRNAs. Both the length and direction of each bar provide crucial insights into the miRNA’s contribution to the first principal component (PC1). The length of the bar signifies the magnitude of the miRNA’s contribution. Longer bars indicate miRNAs that have a more substantial influence on the variance captured by PC1, highlighting them as key elements in distinguishing different patterns of gene expression within the dataset.\nThe direction of the bar adds another layer of interpretation. Bars extending to the right represent miRNAs that are positively correlated with PC1, indicating that as the values of these miRNAs increase, so does the score of PC1. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these miRNAs increase, the score of PC1 decreases. This directional information can be important in understanding the expression patterns of miRNAs in different breast cancer subtypes. For instance, miRNAs that are positively correlated with PC1 might be highly expressed in the Basal subtype but low in others, offering insights into the molecular distinctions between these cancer subtypes."
  },
  {
    "objectID": "mixomics.html#limitations-with-more-data-sets",
    "href": "mixomics.html#limitations-with-more-data-sets",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Limitations with more data sets",
    "text": "Limitations with more data sets\nScore plot together with loading give us powerful tool to investigate patterns in a single data set. But how about if we have multiple data sets? Can we simply go ahead and merge multiple data sets into one and do PCA on this merged data?\nWhile it might be tempting to merge multiple data sets into one and proceed with PCA, this approach has several challenges and limitations. Different data sets can have variations in terms of units, scales, and data collection methods. Simply merging them without addressing these issues can lead to misleading PCA results, where the observed variance is more a reflection of the data sets’ inconsistencies rather than underlying biological patterns. In addition, when data sets are collected at different times, locations, or under different conditions, batch effects can occur. These systematic non-biological differences can confound the PCA results, making it difficult to detect true patterns and relationships within the data. This leads us to multi-omics analysis, where techniques like Canonical Correlation Analysis (CCA) offer ways to detect correlated patterns between two or more omics data sets, and provide a more holistic view of the underlying biological processes."
  },
  {
    "objectID": "mixomics.html#mathematical-foundations",
    "href": "mixomics.html#mathematical-foundations",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Mathematical Foundations",
    "text": "Mathematical Foundations\nCCA seeks to find pairs of linear combinations, one from each dataset, that are maximally correlated. If we have two data sets \\(X\\) and \\(Y\\), the canonical correlations are obtained by solving the optimization problem:\n\\[\n\\max_{a, b} \\rho = \\text{corr}(a^T X, b^T Y)\n\\]\nwhere \\(a\\) and \\(b\\) are the canonical weights, and \\(\\rho\\) is the canonical correlation.\nThe cross-covariance matrix between \\(X\\) and \\(Y\\) plays a central role in calculating CCA weights. We compute it and then apply SVD to find the weights and correlations. The process is similar to performing SVD in PCA but extends to exploring relationships between two data sets. \\[\n\\text{SVD}(\\Sigma_{XY}) = U \\Lambda V^T\n\\]\nwhere \\(\\Sigma_{XY}\\) is the cross-covariance matrix, \\(U\\) and \\(V\\) are the canonical weights for \\(X\\) and \\(Y\\), and \\(\\Lambda\\) contains the canonical correlations.\nWe can now find the canonical variables: \\[\nT_x = XU\n\\] \\[\nT_y = YV\\]\nAfter finding the first pair, we use a deflation process to remove their effect and proceed to find the next pair. This iterative process continues until we extract the desired number of canonical variable pairs.\n\\[\nX = X-T_x(X^TT_U(T^T_UT_U)^{-1})^T\n\\] and for \\(Y\\)\n\\[Y = Y-T_y(Y^TT_U(T^T_UT_U)^{-1})^T \\] The process is repeated to extract additional pairs of canonical variables."
  },
  {
    "objectID": "mixomics.html#cca-in-r",
    "href": "mixomics.html#cca-in-r",
    "title": "Matrix Decomposition for Data Integration",
    "section": "CCA in R",
    "text": "CCA in R\nLet’s have a look at how we can derive this in R using miRNA and mRNA data\n\n# center both of the data sets\nX_centered &lt;- scale(breast.TCGA$data.train$mirna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\n\n# calculate cross-covariance matrix\ncross_cov &lt;- t(X_centered)%*%Y_centered\n\n# do a svd (single eigenvector) this is going to give us a single CCA component\nsvd_result &lt;- svd(cross_cov,1,1)\n\n# extract the vectors\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the first canonical vectors (the most correlated latent factors)\ncanonical_vars_X &lt;- X_centered %*% U\ncanonical_vars_Y &lt;- Y_centered %*% V\n\n# deflate the original matrices\nX_centered &lt;- X_centered - canonical_vars_X %*% t((t(X_centered)%*%(canonical_vars_X)%*%solve(t(canonical_vars_X)%*%(canonical_vars_X))))\n\nY_centered &lt;- Y_centered - canonical_vars_Y %*% \n  t(t(Y_centered)%*%(canonical_vars_Y)%*%solve(t(canonical_vars_Y)%*%(canonical_vars_Y)))\n\n# redo the SVD for the second component\ncross_cov &lt;- t(X_centered)%*%Y_centered\nsvd_result &lt;- svd(cross_cov,1,1)\n\nU &lt;- svd_result$u\nV &lt;- svd_result$v\n\n# calculate the second canonical vectors (the second most correlated latent factors)\ncanonical_vars_X2 &lt;- X_centered %*% U\ncanonical_vars_Y2 &lt;- Y_centered %*% V\n\n# plot\ngraph_scale &lt;- 1.5\npar(mfrow=c(2,2))\nplot(canonical_vars_X, canonical_vars_X2, col=breast.TCGA$data.train$subtype,\n     xlab=\"l1\", ylab=\"l2\", main=\"CCA miRNA\",\n     cex = graph_scale, cex.lab = graph_scale, cex.axis = graph_scale)\n\nplot(canonical_vars_Y, canonical_vars_Y2, col=breast.TCGA$data.train$subtype,\n     xlab=\"l1\", ylab=\"l2\", main=\"CCA mRNA\", \n     cex = graph_scale, cex.lab = graph_scale, cex.axis = graph_scale) \n\nplot(canonical_vars_X, canonical_vars_Y, col=breast.TCGA$data.train$subtype,\n     xlab=\"miRNA\", ylab=\"mRNA\", main=\"l1\", \n     cex = graph_scale, cex.lab = graph_scale, cex.axis = graph_scale)\n\nplot(canonical_vars_X2, canonical_vars_Y2, col=breast.TCGA$data.train$subtype,\n     xlab=\"miRNA\", ylab=\"mRNA\", main=\"l2\",\n     cex = graph_scale, cex.lab = graph_scale, cex.axis = graph_scale)\n\n\n\n\n\n\n\n\nThe plot above clearly shows that we ended up having a shared pattern in l1 (first CCA component). L1 captures the primary mode of correlation between miRNA and mRNA expression data. It represents the linear combinations of miRNAs and mRNAs that are most strongly correlated. Since our interest right now is in the suptypes, we can probably ignore the second latent factor but we might as well try to explain if based on some other factors.\nWe are going to explore the loadings later when we explain multiomics CCA i.e. GCCA but for now remember that in the context of CCA, loadings play a role similar to that in PCA, yet they have a distinct interpretation. Similar to PCA, where loadings indicate the contribution of each original variable to the principal components, in CCA, the loadings show the contribution of each variable to the canonical variables. However, the difference lies in their meaning. While PCA loadings represent the contribution to the variance within a single data set, CCA loadings show the contribution to the correlation between two datasets.\nSo far we have been going through CCA using two data sets. Can it be expaned to more data sets? Yes, via Generalized Canonical Correlation Analysis."
  },
  {
    "objectID": "mixomics.html#partial-least-squares",
    "href": "mixomics.html#partial-least-squares",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\nWe are now going to formally define an outcome variable and use that to perform supervised data integration. This is where Partial Least Squares (PLS) comes into play, a method that not only facilitates the integration of data from various sources but also enables the prediction of an outcome variable by identifying the relationships between observed variables and the outcomes of interest.\nPLS is a statistical method used in the context of predictive modeling and data analysis. It serves as a bridge between principal component analysis (PCA) and regression analysis. PLS is particularly useful when dealing with complex, high-dimensional, and multicollinear data, where traditional regression models may falter due to overfitting or multicollinearity issues.\nIn PLS, the predictor variables (or features) and the response variables (or outcomes) are projected to a new subspace formed by latent variables (or components). These latent variables are linear combinations of the original variables and are constructed in such a way that they maximize the covariance between the predictors and the response. This is a key distinction from PCA, which only considers the variance of the predictor variables.\nThe PLS model aims to find the optimal set of weights that, when applied to the original variables, gives the best possible prediction of the outcome variable. It does this by decomposing both the predictor matrix \\(X\\) and the response matrix \\(Y\\) into the product of two lower-dimensional matrices, capturing the most relevant information in the data for predicting outcomes.\nMathematically, the decomposition can be represented as: \\[\nX = T P^T + E\n\\] \\[\nY = U Q^T + F\\]\nwhere:\n\n\\(X\\) is the matrix of predictor variables,\n\\(Y\\) is the matrix of response variables,\n\\(T\\) and \\(U\\) are matrices of scores representing the latent variables,\n\\(P\\) and \\(Q\\) are matrices of loadings,\n\\(E\\) and \\(F\\) are matrices of residuals.\n\nMore specifically, there are two main differences between CCA and PLS when it comes to underlining equations. In CCA, we first calculate each covariance matrix for each dataset and then continue with the deflation. In contrast, in PLS, we need to calculate the cross covariance between \\(X\\) and \\(Y\\) followed by SVD and deflation.\nThe second difference is the deflation procedure itself. The main difference lies in the scores used for deflation. In CCA, each block is deflated by its own scores, while in PLS, both blocks are deflated by the \\(X\\) scores, ensuring that the shared information captured is based on the covariance between \\(X\\) and \\(Y\\).\nIn CCA we have:\n\\[\nX' = X - T_X (X^T T_X) (T_X^T T_X)^{-1}\n\\] \\[\nY' = Y - T_Y (Y^T T_Y) (T_Y^T T_Y)^{-1}\n\\]\nwhere:\n\n\\(T_Y\\) is the matrix of scores associated with \\(Y\\).\n\nIn PLS deflation:\n\\[X' = X - T_X (X^T T_X) (T_X^T T_X)^{-1}\\]\n\\[Y' = Y - T_X (Y^T T_X) (T_X^T T_X)^{-1}\\]\nwhere:\n\n\\(X'\\) and \\(Y'\\) are the deflated \\(X\\) and \\(Y\\) blocks respectively.\n\\(T_X\\) is the matrix of scores associated with \\(X\\).\nThe operation \\(X^T T_X\\) and \\(Y^T T_X\\) calculates the projection of \\(X\\) and \\(Y\\) on the scores \\(T_X\\).\nThe term \\((T_X^T T_X)^{-1}\\) is the inverse of the matrix resulting from the multiplication of \\(T_X^T\\) and \\(T_X\\), used for normalization.\n\nTo see this in practice we will change our R code and do PLS instead of CCA. In the following example, we are going to using our mirna data as X and protein as Y. It is important to note that one of the most amazing properties of PLS is the capability to regress on multivariate outcome in contrast to ordinary regression which often involves a single outcome.\n\n# center the data\nX_centered &lt;- scale(breast.TCGA$data.train$mirna, scale = FALSE)\nZ_centered &lt;- scale(breast.TCGA$data.train$protein, scale = FALSE)\n\n# define y index\ny_index &lt;- 2\n\n# add them to a list for easier access\ndata_merged&lt;-list(mirna=X_centered, y=Z_centered)\n\n# define the number of components\nnumber_of_components &lt;- 2\n\nscores_list&lt;-list()\nfor(cmp in 1:number_of_components)\n{\n# initialize the loadings based on SVD how cross covariance matrix\ninitial_loadings &lt;- lapply(data_merged,function(x){svd(t(x)%*%data_merged[[y_index]],1,1)$u})\ninitial_loadings[[length(initial_loadings)]] &lt;- lapply(data_merged,function(x){svd(t(x)%*%data_merged[[2]],1,1)$v})[[1]]\n# define an empty loading for new ones\nnew_loadings &lt;- initial_loadings\nnew_loadings[] &lt;- NA\n\n# calculate the PC scores\npc_scores &lt;- mapply(function(x,y){x%*%y},x=data_merged,y=initial_loadings)\nrepeat{\n  \n# dataset index\ndata_index &lt;- 1:length(data_merged)\n\n# for each dataset\nfor(i in 1:length(data_merged))\n{\n  # re-estimate the loadings based on the regression on the scores of the other dataset\n  new_loadings[[i]] &lt;- crossprod(data_merged[[i]], \n                  rowSums(pc_scores[,data_index!=i,drop=F])) ## row sum will calculate the sum of all the scores for each data point across different data sets\n  # normalize the loadings \n  new_loadings[[i]] = new_loadings[[i]]/drop(sqrt(crossprod(new_loadings[[i]])))\n  \n  # update the scores\n  pc_scores[, i] = data_merged[[i]] %*% new_loadings[[i]]\n  \n}\n\nscores_list[[cmp]] &lt;- pc_scores\n\n# calculate epsilon\nepi &lt;- max(sapply(1:length(data_merged), function(x) {\n            crossprod(new_loadings[[x]] - initial_loadings[[x]])\n        }))\n\nif(epi&lt;.Machine$double.eps)\n  break\n# update the old loadings\ninitial_loadings = new_loadings\n}\n\n# perform deflation on X first\ndata_merged[-y_index]&lt;-lapply((1:length(data_merged))[-y_index],function(x){\n  x_tmp &lt;- data_merged[[x]]\n  x_tmp - pc_scores[,x,drop=F] %*% t((t(x_tmp)%*%(pc_scores[,x,drop=F])%*%solve(t(pc_scores[,x,drop=F])%*%(pc_scores[,x,drop=F]))))\n  \n  })\n\n# perform deflation\ndata_merged[y_index]&lt;-\n  lapply(y_index,function(x){\n  x_tmp &lt;- data_merged[[x]]\n  x_tmp - pc_scores[,-x,drop=F] %*% t((t(x_tmp)%*%(pc_scores[,-x,drop=F])%*%solve(t(pc_scores[,-x,drop=F])%*%(pc_scores[,-x,drop=F]))))\n  \n  })\n\n}\n\n# plot\npar(mfrow=c(1,1))\nplot(scores_list[[1]][,1],scores_list[[2]][,1],\n     col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"PLS miRNA\")\n\n\n\n\n\n\n\n\nWe ended up with the single pairs of scores which maximize the covariance between miRNA and protein data sets. You can think about PLS as ordinary regression where both the predictor and response variables are simultaneously transformed to a new space defined by latent variables. These latent variables are constructed to maximize the covariance between the transformed predictor and response variables, ensuring that the most relevant features for prediction are captured. Unlike ordinary regression, which can struggle with multicollinearity and high-dimensional data, PLS handles these issues efficiently by reducing the dimensionality and focusing on the most informative components of the data."
  },
  {
    "objectID": "mixomics.html#partial-least-squares-extending-to-k-blocks",
    "href": "mixomics.html#partial-least-squares-extending-to-k-blocks",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Partial Least Squares (extending to \\(k\\) blocks)",
    "text": "Partial Least Squares (extending to \\(k\\) blocks)\nAs you have noticed, PLS algorithm is not an integrative method. It is still applied on a single block of data, miRNA in the above example and with protein data set used as Y block. We can however extend PLS to multiple blocks of data. We keep what we have done for CCA. The only part that we have to change is the deflation of \\(Y\\). As we said given the block \\(X\\) and \\(Y\\) the deflation of \\(Y\\) is based on the information captured in \\(X\\), meaning its scores \\(T\\). When it comes to multiple omics (data views), we do not have a single matrix of \\(X\\) but rather we have \\(X_k\\) where \\(k\\) can be \\(1...K\\), each being a separate dataset. A simple way to do the deflation of \\(Y\\) relative to all the \\(X_k\\) is to deflate the \\(Y\\) for each dataset separately and then take the average of all deflation. More concretely,\n\\[\n\\Delta Y_k = Y - T_{x_k} \\cdot \\left( Y^T \\cdot T_{x_k} \\cdot (T_{x_k}^T \\cdot T_{x_k})^{-1} \\right)^T\n\\]\nwhere\n\n\\(Y\\) is the response matrix,\n\\(T_{x_k}\\) is the score matrix corresponding to the \\(k\\)-th block of predictors,\n\\(\\Delta Y_k\\) is the deflated \\(Y\\) corresponding to the \\(k\\)-th block of predictors.\n\nThen, the final deflated \\(Y\\) is obtained by averaging all the individual deflations: \\[\n\\Delta Y_{\\text{final}} = \\frac{1}{K} \\sum_{k=1}^K \\Delta Y_k\n\\]\nThis ensures that the deflation of \\(Y\\) takes into account the information captured in all blocks of predictors, leading to a more comprehensive and integrative analysis when dealing with multiple omics or data views.\nWe can do a simple change to our previous code to do that.\n\n# center the data\nX_centered &lt;- scale(breast.TCGA$data.train$mirna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\nZ_centered &lt;- scale(breast.TCGA$data.train$protein, scale = FALSE)\n\n# define y index\ny_index &lt;- 3\n\n# add them to a list for easier access\ndata_merged&lt;-list(mirna=X_centered,mrna=Y_centered,y=Z_centered)\n\n# define the number of components\nnumber_of_components &lt;- 2\n\nscores_list&lt;-list()\nfor(cmp in 1:number_of_components)\n{\n  \n# initialize the loadings based on SVD how cross covariance matrix\ninitial_loadings &lt;- lapply(data_merged,function(x){svd(t(x)%*%data_merged[[y_index]],1,1)$u})\ninitial_loadings[[y_index]] &lt;- lapply(data_merged,function(x){svd(t(x)%*%data_merged[[y_index]],1,1)$v})[[1]]\n\n# define an empty loading for new ones\nnew_loadings&lt;-initial_loadings\nnew_loadings[]&lt;-NA\n\n# calculate the PC scores\npc_scores &lt;- mapply(function(x,y){x%*%y},x=data_merged,y=initial_loadings)\nrepeat{\n  \n# dataset index\ndata_index&lt;-1:length(data_merged)\n# for each dataset\nfor(i in 1:length(data_merged))\n{\n  # re-estimate the loadings based on the regression on the scores of the other dataset\n  new_loadings[[i]] &lt;- crossprod(data_merged[[i]], \n                  rowSums(pc_scores[,data_index!=i,drop=F])) ## row sum will calculate the sum of all the scores for each data point across different data sets\n  # normalize the loadings \n  new_loadings[[i]] = new_loadings[[i]]/drop(sqrt(crossprod(new_loadings[[i]])))\n  \n  # update the scores\n  pc_scores[, i] = data_merged[[i]] %*% new_loadings[[i]]\n  \n}\nscores_list[[cmp]]&lt;-pc_scores\n# calculate epsilon\nepi &lt;- max(sapply(1:length(data_merged), function(x) {\n            crossprod(new_loadings[[x]] - initial_loadings[[x]])\n        }))\n\nif(epi&lt;.Machine$double.eps)\n  break\n# update the old loadings\ninitial_loadings = new_loadings\n}\n\n# perform deflation on X first\ndata_merged[-y_index]&lt;-lapply((1:length(data_merged))[-y_index],function(x){\n  x_tmp &lt;- data_merged[[x]]\n  x_tmp - pc_scores[,x,drop=F] %*% t((t(x_tmp)%*%(pc_scores[,x,drop=F])%*%solve(t(pc_scores[,x,drop=F])%*%(pc_scores[,x,drop=F]))))\n  \n  })\n\n# perform deflation of Y which has been changed now!\ndata_merged[[y_index]]&lt;-Reduce(\"+\",\n  lapply(seq(1,length(data_merged))[-y_index],function(x){\n  x_tmp &lt;- data_merged[[y_index]]\n  x_tmp - pc_scores[,x,drop=F] %*% t((t(x_tmp)%*%(pc_scores[,x,drop=F])%*%solve(t(pc_scores[,x,drop=F])%*%(pc_scores[,x,drop=F]))))\n  \n  }))/(length(data_merged)-1)\n\n}\n\n\npar(mfrow=c(1,2))\nplot(scores_list[[1]][,1],scores_list[[2]][,1],\n     col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"PLS miRNA\")\nplot(scores_list[[1]][,2],scores_list[[2]][,2],\n     col=breast.TCGA$data.train$subtype,xlab=\"l1\",ylab=\"l2\",main=\"PLS mRNA\")\n\n\n\n\n\n\n\n\nUsing mixOmics one can perform multiblock PLS using block.pls function.\n\n# center the data\nX_centered &lt;- scale(breast.TCGA$data.train$mirna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\nZ_centered &lt;- scale(breast.TCGA$data.train$protein, scale = FALSE)\n\n# define y index\ny_index &lt;- 3\n\n# add them to a list for easier access\ndata_merged&lt;-list(mirna=X_centered, mrna=Y_centered, y=Z_centered)\n\n# perform PLS\nblock_pls_results &lt;- mixOmics::block.pls(data_merged, indY = y_index, scale = FALSE, ncomp = 2)\n\n# plot the results\nmixOmics::plotIndiv(block_pls_results,group = breast.TCGA$data.train$subtype,\n                    ind.names = F,\n                    legend = T, legend.position = \"top\")\n\n\n\n\n\n\n\n\nAs you have noted, we have fitted a supervised regression model that uses miRNA and mRNA data sets to predict protein expression. Regression models are used when we have a continuous response variable. How about if instead of protein expression we wanted to predict cancer subtype using miRNA and mRNA data? Cancer subtype is categorical which does not follow the assumption of the PLS.\nFortunately there is a simple trick that can be used to perform classification using PLS which turns PLS into PLS-DA (Partial Least Squares Discriminant Analysis)."
  },
  {
    "objectID": "mixomics.html#pls-da",
    "href": "mixomics.html#pls-da",
    "title": "Matrix Decomposition for Data Integration",
    "section": "PLS-DA",
    "text": "PLS-DA\nThe simple trick to use PLS to do classification is to convert our categorical response to a numerical matrix. We can then do standard PLS using this matrix as \\(Y\\). If you have a categorical outcome variable \\(Y\\) with \\(K\\) levels, you can convert it into a dummy matrix \\(D\\) using the following mathematical representation. Let \\(Y = \\{y_1, y_2, \\ldots, y_n\\}\\) be a vector of \\(n\\) observations of the categorical variable, where each \\(y_i\\) can take on one of \\(K\\) distinct values or levels. The dummy matrix \\(D\\) is then an \\(n \\times K\\) matrix defined as\n\\[\nD_{ij} =\n\\begin{cases}\n1 & \\text{if } y_i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nfor \\(i = 1, 2, \\ldots, n\\) and \\(j = 1, 2, \\ldots, K\\).\nEach column of \\(D\\) corresponds to one level of the categorical variable \\(Y\\), and each row corresponds to an observation. The elements of \\(D\\) are binary indicators, with a 1 indicating the presence of the corresponding level for that observation and 0 indicating the absence.\nHere is a code to do that!\n\n# create a factor variable\nY &lt;- breast.TCGA$data.train$subtype\n\n# get the levels of the factor\nlevels &lt;- unique(Y)\n\n# create a dummy matrix\ndummy_matrix &lt;- matrix(0, nrow = length(Y), ncol = length(levels))\ncolnames(dummy_matrix) &lt;- levels\n\n# fill in the dummy matrix\nfor (i in seq_along(levels)) {\n  dummy_matrix[, i] &lt;- as.integer(Y == levels[i])\n}\n\n# print the dummy matrix\nprint(dummy_matrix)\n\n       Basal Her2 LumA\n  [1,]     1    0    0\n  [2,]     1    0    0\n  [3,]     1    0    0\n  [4,]     1    0    0\n  [5,]     1    0    0\n  [6,]     1    0    0\n  [7,]     1    0    0\n  [8,]     1    0    0\n  [9,]     1    0    0\n [10,]     1    0    0\n [11,]     1    0    0\n [12,]     1    0    0\n [13,]     1    0    0\n [14,]     1    0    0\n [15,]     1    0    0\n [16,]     1    0    0\n [17,]     1    0    0\n [18,]     1    0    0\n [19,]     1    0    0\n [20,]     1    0    0\n [21,]     1    0    0\n [22,]     1    0    0\n [23,]     1    0    0\n [24,]     1    0    0\n [25,]     1    0    0\n [26,]     1    0    0\n [27,]     1    0    0\n [28,]     1    0    0\n [29,]     1    0    0\n [30,]     1    0    0\n [31,]     1    0    0\n [32,]     1    0    0\n [33,]     1    0    0\n [34,]     1    0    0\n [35,]     1    0    0\n [36,]     1    0    0\n [37,]     1    0    0\n [38,]     1    0    0\n [39,]     1    0    0\n [40,]     1    0    0\n [41,]     1    0    0\n [42,]     1    0    0\n [43,]     1    0    0\n [44,]     1    0    0\n [45,]     1    0    0\n [46,]     0    1    0\n [47,]     0    1    0\n [48,]     0    1    0\n [49,]     0    1    0\n [50,]     0    1    0\n [51,]     0    1    0\n [52,]     0    1    0\n [53,]     0    1    0\n [54,]     0    1    0\n [55,]     0    1    0\n [56,]     0    1    0\n [57,]     0    1    0\n [58,]     0    1    0\n [59,]     0    1    0\n [60,]     0    1    0\n [61,]     0    1    0\n [62,]     0    1    0\n [63,]     0    1    0\n [64,]     0    1    0\n [65,]     0    1    0\n [66,]     0    1    0\n [67,]     0    1    0\n [68,]     0    1    0\n [69,]     0    1    0\n [70,]     0    1    0\n [71,]     0    1    0\n [72,]     0    1    0\n [73,]     0    1    0\n [74,]     0    1    0\n [75,]     0    1    0\n [76,]     0    0    1\n [77,]     0    0    1\n [78,]     0    0    1\n [79,]     0    0    1\n [80,]     0    0    1\n [81,]     0    0    1\n [82,]     0    0    1\n [83,]     0    0    1\n [84,]     0    0    1\n [85,]     0    0    1\n [86,]     0    0    1\n [87,]     0    0    1\n [88,]     0    0    1\n [89,]     0    0    1\n [90,]     0    0    1\n [91,]     0    0    1\n [92,]     0    0    1\n [93,]     0    0    1\n [94,]     0    0    1\n [95,]     0    0    1\n [96,]     0    0    1\n [97,]     0    0    1\n [98,]     0    0    1\n [99,]     0    0    1\n[100,]     0    0    1\n[101,]     0    0    1\n[102,]     0    0    1\n[103,]     0    0    1\n[104,]     0    0    1\n[105,]     0    0    1\n[106,]     0    0    1\n[107,]     0    0    1\n[108,]     0    0    1\n[109,]     0    0    1\n[110,]     0    0    1\n[111,]     0    0    1\n[112,]     0    0    1\n[113,]     0    0    1\n[114,]     0    0    1\n[115,]     0    0    1\n[116,]     0    0    1\n[117,]     0    0    1\n[118,]     0    0    1\n[119,]     0    0    1\n[120,]     0    0    1\n[121,]     0    0    1\n[122,]     0    0    1\n[123,]     0    0    1\n[124,]     0    0    1\n[125,]     0    0    1\n[126,]     0    0    1\n[127,]     0    0    1\n[128,]     0    0    1\n[129,]     0    0    1\n[130,]     0    0    1\n[131,]     0    0    1\n[132,]     0    0    1\n[133,]     0    0    1\n[134,]     0    0    1\n[135,]     0    0    1\n[136,]     0    0    1\n[137,]     0    0    1\n[138,]     0    0    1\n[139,]     0    0    1\n[140,]     0    0    1\n[141,]     0    0    1\n[142,]     0    0    1\n[143,]     0    0    1\n[144,]     0    0    1\n[145,]     0    0    1\n[146,]     0    0    1\n[147,]     0    0    1\n[148,]     0    0    1\n[149,]     0    0    1\n[150,]     0    0    1\n\n\nWe can now do PLS on this matrix which is equal to doing PLS-DA.\n\n# center the data\nX_centered &lt;- scale(breast.TCGA$data.train$mirna, scale = FALSE)\nY_centered &lt;- scale(breast.TCGA$data.train$mrna, scale = FALSE)\nZ_centered &lt;- scale(dummy_matrix, scale = FALSE) # note this has replaced protein expression\n\nrownames(Z_centered) &lt;- rownames(Y_centered)\n\n# define y index\ny_index &lt;- 3\n\n# add them to a list for easier access\ndata_merged&lt;-list(mirna=X_centered,mrna=Y_centered,y=Z_centered)\n\n# perform PLS\nblock_plsda_results &lt;- mixOmics::block.pls(data_merged,indY = y_index,scale = FALSE,ncomp = 2)\n\n# plot the results\nmixOmics::plotIndiv(block_pls_results, group = breast.TCGA$data.train$subtype,\n                    ind.names = F,\n                    legend = T,\n                    legend.position = \"top\",\n                    blocks = c(1,2))\n\n\n\n\n\n\n\n\nNow it comes a critical question. Given that we have done regression, meaning that we can only predict continuous values, how can we transform these continuous values to our cancer subtypes groups, i.e. categorical values?\nImagine that we have a single test sample for which we want to predict the class. One way to do so is to project that sample (using the PLS weights) onto the PLS space (scores) and then look what groups of samples are “closer” to the projected sample. We can then assign the closer group label to that sample.\nWe can do that following this notation:\n\nCompute the projection matrix \\(P\\): \\[\nP = X_i^T \\cdot V_i\n\\] where\n\n\n\\(X_i\\): the \\(i\\)-th data matrix,\n\\(V_i\\): the \\(i\\)-th variates matrix.\n\n\nCompute the weight matrix \\(W\\): \\[\nW = L_i\n\\] where\n\n\n\\(L_i\\): the \\(i\\)-th loadings matrix.\n\n\nCalculate the predicted scores \\(T_{\\text{pred}}\\): \\[\nT_{\\text{pred},i} = X_{\\text{new},i} \\cdot W \\cdot (P^T \\cdot W)^{-1}\n\\] where\n\n\n\\(X_{\\text{new},i}\\): the \\(i\\)-th new data matrix to be projected.\n\n\nNormalize the predicted scores: \\[\nT_{\\text{pred},i} = T_{\\text{pred},i} \\cdot \\text{diag}\\left(\\|V_{i,j}\\|_2^2\\right)^{-1}\n\\] where\n\n\n\\(\\|V_{i,j}\\|_2^2\\): the squared Euclidean norm of the \\(j\\)-th column of \\(V_i\\),\n\\(\\text{diag}(\\cdot)\\): a diagonal matrix formed from the vector of squared norms.\n\nLet’s try to implement that.\n\n# construct the data\ndata_merged_test &lt;- list(mirna=breast.TCGA$data.test$mirna[1,,drop=F],\n                         mrna=breast.TCGA$data.test$mrna[1,,drop=F])\n\n# center using the training dataset\ndata_merged_test &lt;- lapply(1:length(data_merged_test), \n                  function(x) {\n                    sweep(data_merged_test[[x]], 2, STATS = attr(data_merged[[x]], \n                      \"scaled:center\"))\n                  })\nnames(data_merged_test)&lt;-c(\"mirna\",\"mrna\")\n\n# estimate the loadings\nloading_matrix = lapply(1:length(data_merged_test),function(i){\n  \n  crossprod(block_plsda_results$X[[i]], block_plsda_results$variates[[i]])\n})\n\n# extract the weights\nweights = block_plsda_results$loadings[-y_index]\n\n# calculate the projection\nprojections &lt;- lapply(1:length(data_merged_test),function(i){\n  \n  # unscaleled projection\n  unscaled_prj &lt;- data_merged_test[[i]]%*%weights[[i]]%*%solve(t(loading_matrix[[i]]) %*% \n            weights[[i]])\n  \n  # calculate scaller as 2-norm\n  scaller &lt;- apply(block_plsda_results$variates[[i]], 2, function(y) {\n                  (norm(y, type = \"2\"))^2\n                })\n  \n  # re-scale the projection\n  sweep(unscaled_prj, 2, scaller, \"*\")\n\n})\n\n# plot the projection  \n# sssign colors to each subtype\ncolors &lt;- c(\"red\", \"green\", \"purple\",\"blue\")\nnames(colors) &lt;- levels(breast.TCGA$data.train$subtype)\n\npar(mfrow=c(1,2))\nplot(block_plsda_results$variates$mirna, col=colors[breast.TCGA$data.train$subtype], main=\"miRNA scores\")\npoints(projections[[1]], col=\"blue\", pch=16, cex=3)\nlegend(\"bottomleft\", legend=c(levels(breast.TCGA$data.train$subtype), \"projected point\"), fill=colors, title=\"Subtype\")\n\nplot(block_plsda_results$variates$mrna, col=colors[breast.TCGA$data.train$subtype], main=\"mRNA scores\")\npoints(projections[[2]], col=\"blue\", pch=16, cex=3)\nlegend(\"bottomleft\", legend=c(levels(breast.TCGA$data.train$subtype), \"projected point\"), fill=colors, title=\"Subtype\")\n\n\n\n\n\n\n\n\n\nPLS-DA\nIn the plot above, we can see the projection of the test data point to the PLS space.\n\nWhich group does it belong to?\nCan you check based on the data if your prediction is correct?\n\n\n\n\nCode\n#1. \n#Based on the plot we can see that the point belong to the Basal subtype. \n\n#2. \n# To check if the prediction is correct, we look at the subtype information for this point in the test set. We were using the first data point, and to extract its label we can do: \n\nprint(breast.TCGA$data.test$subtype[1])\n\n# and we see that our prediction is correct.\n\n\nIn the similar way, we can predict whatever number of new samples we want to. We can use distance measurement to automate the process of making predictions.\nWe now know most of ingredients of PLS, PLS-DA and multiomics data integration using PLS. Now we can see how these methods are used as part of the mixOmics package."
  },
  {
    "objectID": "mixomics.html#diablo",
    "href": "mixomics.html#diablo",
    "title": "Matrix Decomposition for Data Integration",
    "section": "DIABLO",
    "text": "DIABLO\nDIABLO is the name of the method that implements the PLS based process that we have just gone through.\nIn order to do DIABLO, similar as before we need to create a list which contain our data:\n\n# define training data\ntraining_data = list(miRNA = breast.TCGA$data.train$mirna, \n            mRNA = breast.TCGA$data.train$mrna)\n\nY_training &lt;- breast.TCGA$data.train$subtype\n\nHere we skipped centering etc. because DIABLO is going to do that for us.\nWe need one more thing before doing DIABLO and that is the design matrix. As discuss before:\nThink of design as a grid where each cell’s value, ranging from 0 to 1, indicates the strength of the relationship between two corresponding data blocks. A value of 0 means no relationship, while 1 signifies a strong connection. When you are setting up your analysis, adjusting these values can help you emphasize or de-emphasize certain relationships, giving you the flexibility to focus on specific interactions that are of interest.\nChoosing the magnitude of the relationship is not straightforward. It for example can be from previous experiments or so. But generally choosing a very high value will have negative impact on the prediction ability of the model. Choosing a very low value on the other hand will cause discarding the relationship between the data blocks. Here we randomly choose to go with 0.2 to describe the strength of relationship between miRNA and RNA. In practice one might want to do Sensitivity Analysis or Cross-Validation to try to optimize this value.\n\n# define square matrix filled with 0.2\ndesign = matrix(0.2, ncol = length(training_data), nrow = length(training_data), \n                dimnames = list(names(training_data), names(training_data)))\ndiag(design) = 0 # set diagonal to 0s\n\nprint(design)\n\n      miRNA mRNA\nmiRNA   0.0  0.2\nmRNA    0.2  0.0\n\n\nWe can proceed with running the analysis\n\nlibrary(mixOmics)\ndiablo.model = block.splsda(X = training_data, Y = Y_training, ncomp = 2, design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\nRight now we have a model that includes two components. We can go ahead and visualize the PLS scores.\n\n# plot results\nplotIndiv(diablo.model,ind.names = F,\n          legend = T, legend.position = \"top\")\n\n\n\n\n\n\n\n\nWe see three separate clusters formed in the PLS space in both OMICS. That looks great! But this can be misleading. We need to be able to systematically say how good our model’s performance is before making any interpretation. This means we need to check whethever we have fitted the data well enough or wherever we have overfitted the data.\nOverfitting occurs when a model is too complex, capturing noise in the training data and making it less effective in generalizing to new, unseen data. It essentially “memorizes” the training data, leading to excellent performance on that specific dataset but poor performance on a new data set. To mitigate overfitting and evaluate the model’s performance accurately, we employ techniques like cross-validation. Cross-validation involves partitioning the original training data set into a training set and a validation set. The model is then trained (build) on the training set and evaluated on the validation set. This process is repeated several times, with different partitions, to obtain a more generalized model performance metric. Techniques like k-fold cross-validation, where the original training set is divided into k subsets and the model is trained k times, each time using a different subset as the validation set, are commonly used. This rigorous evaluation process helps in assessing the model’s ability to generalize, ensuring that it performs well not just on the training data, but on unseen data as well. For more information see here.\nmixOmics provides us with a function (perf) to evaluate the model using cross-validation. We are going to do a 10-fold cross-validation and repeat it 10 times. This means that the entire dataset is divided into 10 equal parts, or “folds”. In each iteration, 9 folds are used for training the model, and the remaining fold is used for testing. This process is repeated 10 times, with each fold getting a chance to be the test set. The model’s performance is then averaged over all the iterations to get a more stable and reliable estimate of its performance. Repeating the entire 10-fold cross-validation process 10 times helps in reducing the variability associated with the random splitting of data and provides a more robust measure of the model’s accuracy and generalizability. Each repetition involves a different random split of the data into folds, ensuring diverse training and testing combinations and thus, a comprehensive evaluation.\n\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 2, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nAlright. This plots gives us a lot of information. The colors represent which distance measure has been used to assign groups to the test data. As we discussed we can use different distance measurement to predict the classes from the PLS continuous values. In practice these distances should do very similar but one might want to focus on centroid and mahalanobis distance more.\nThe type of the line (solid and dashed) shown error rate (ER) and balanced error rate (BER) respectively. Error Rate (ER) is the proportion of incorrect predictions made by the model over the total number of predictions, while Balanced Error Rate (BER) is the average error rates across all classes, ensuring that each class contributes equally to the overall error rate, even if the classes are imbalanced in size. In the plot, different line types allow for an easy visual comparison between these two metrics for each distance measure.\nThe x-axis represents the number of components included in the model, which is a crucial aspect of PLS-DA. By observing how the error rates change with the inclusion of additional components, one can identify the optimal number of components that minimizes the error rates, striking a balance between model complexity and predictive accuracy.\nIn evaluating these plots, one should look for the point where both ER and BER are minimized, indicating optimal model performance. Special attention should be given to the results obtained using centroid and Mahalanobis distances, as they often provide a more nuanced and reliable classification, especially in complex multiclass scenarios or when the data distribution is not straightforward. By comparing the error rates associated with different distance measures and numbers of components, one can fine-tune the PLS-DA model for optimal classification performance.\nperf functions gives a lot of other information but probably the most useful ones are:\n\nauc: Averaged AUC values over the nrepeat (if requested)\nMajorityVote.error.rate: ff more than one block, returns the error rate of the MajorityVote output\n\nHave a look at the help page of the function to know more.\nNow we know that we do actually fairly good because low error rate, meaning that the model is usable.\n\nExercise 2 (DIABLO) Can we make the above model better? Try changing the design matrix to different values, e.g. 0.8 or 0.1 and compare the results of cross-validation.\n\n\n\nShow the code\n# update design matrix\ndesign_new &lt;- design\ndesign_new[1,2] &lt;- design_new[2,1] &lt;- 0.1\n\n# re-run DIABLO\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 2, \n                            design = design_new) \n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n# Comment: \n# In this case selecting lower values of connection between mRNA and miRNA yields smaller ER and BER with 2 components\n# and selecting high values of 0.8 yields higher ER and BER with 2 components. \n\n\n\nTuning number of components\nI guess you also asked yourself why we should use two components. Maybe we can go higher? Or lower? We can try to tune number of components by simply including more components in the model and redoing the cross-validation.\n\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 5, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nThis is the same plot as before but now we have more component on the x-axis. What this plot tells us that the error rate sharply decreases from ~0.3 to 0.1 just by including two components instead of one. Then we have smaller decrease from component two to three. After that the improvement is very little. So shall we should use 3 components or five? This is where the trade-off between model complexity and performance comes into play. While adding more components can lead to a slight decrease in error rate, it also increases the complexity of the model, which can lead to more difficulty in interpretation etc. So we should go for the least complex model that gives us a reasonable error rate.\nmixOmics gives us an automated way of selecting the best component. We can see mixOmics choice using:\n\nprint(perf.diablo$choice.ncomp)\n\n$AveragedPredict\n            [,1]\nOverall.ER     4\nOverall.BER    4\n\n$WeightedPredict\n            [,1]\nOverall.ER     3\nOverall.BER    3\n\n$MajorityVote\n            max.dist centroids.dist mahalanobis.dist\nOverall.ER         5              3                3\nOverall.BER        4              3                3\n\n$WeightedVote\n            max.dist centroids.dist mahalanobis.dist\nOverall.ER         3              3                4\nOverall.BER        3              3                4\n\n\nI this case, we are going to use majority vote and mahalanobis.dist so we we choose three components and redo the analysis.\n\nncomps &lt;- perf.diablo$choice.ncomp$MajorityVote[\"Overall.BER\",\"centroids.dist\"]\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = ncomps, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\n\n\nVariable selection\nWe now have a working model with a reasonably low error rate. We can use this model to extract the most important features (e.g. miRNA or mRNA) that gave us such a discriminatory pattern we saw in the score plots.\nWe should do that three times in fact. One for each components. We have one set of loading for each component and for each block of data. it is quite a lot of work to go through these loadings. In the following plots, the bars have a color corresponding the group which has the maximum (max in contrib) of average (mean in method) among all the groups.\n\nplotLoadings(diablo.model, comp=1, contrib='max', method='mean')\n\n\n\n\n\n\n\nplotLoadings(diablo.model, comp=2, contrib='max', method='mean')\n\n\n\n\n\n\n\nplotLoadings(diablo.model, comp=3, contrib='max', method='mean')\n\n\n\n\n\n\n\n\nSimilar to what we said before, the most important variables, according to the absolute value of their coefficients, are ordered from bottom to top. One can go ahead and examine top \\(n\\) variables, e.g. 10. They have the same interpretation as before: bars extending to the right represent features that are positively correlated with PC1, indicating that as the values of these features increase, so does the score of corresponding component. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these features increase, the score of the component decreases.\nIn this specific example we have a limited number of features so it might be feasible to do it manually. However, in practice most omics data sets are large and that will make it very difficult to choose a right cutoff to select the number of features per omics.\nFortunately, mixOmics gives us an automated way of selecting variables using tune.block.splsda function. The only thing we have to do is to select a grid of plausible values for each component to test. mixOmics will then do cross-validation and try to give us the most influential variables on the model and discard the rest. This means that, given a vector of possible number of variables, the function will pinpoint the optimal subset that contributes the most to our model’s predictive power, ensuring both efficiency and accuracy in our analysis.\n\n# set grid of plausible values for each component to test\ntest.keepX = list (mirna =seq(10, 18, 2),\n                  mrna = seq(10, 18, 2))\n                   \n# run the feature selection tuning\nset.seed(123)\ntune.TCGA = tune.block.splsda( training_data, Y = Y_training, ncomp = ncomps, \n                              test.keepX = test.keepX, design = design,\n                              validation = 'Mfold', folds = 10, nrepeat = 1,\n                              dist = \"centroids.dist\")\n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\n\nYou have provided a sequence of keepX of length: 5 for block mirna and 5 for block mrna.\nThis results in 25 models being fitted for each component and each nrepeat, this may take some time to run, be patient!\n\n\n\nYou can look into the 'BPPARAM' argument to speed up computation time.\n\nplot(tune.TCGA)\n\n\n\n\n\n\n\n\nThis bar plot can help us seeing the process of variable selection. In each component the x-axis is BER and y-axis is the combination of the number of selected variable in omics1 (mirna) and omics2 (mrna) which has been written as omics1_omics2 (for example 10_12 means 10 variables from mirna and 12 variables from mrna block). Again we are going to look for the least complex (fewer variables) model with lower BER. In component one the top 5 selections are in the top of the plot, the least numbers among them is 10_16, similarly we can select 10_14 for the second component and 10_18 for the last one. mixOmics also gives us these numbers in choice.keepX list.\n\nprint(tune.TCGA$choice.keepX)\n\n$miRNA\n[1] 10 10 10\n\n$mRNA\n[1] 16 14 18\n\n\nNow that we have the number of variables we can go ahead and do the final model:\n\n# set the optimised DIABLO model\nfinal.diablo.model = block.splsda(training_data, \n                                  Y = Y_training, \n                                  ncomp = ncomps,\n                                  keepX = tune.TCGA$choice.keepX, \n                                  design = design)\n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\nNow for each components we can see which variables have been selected. For example for component 1.\n\nselectVar(final.diablo.model, block = 'miRNA', comp = 1)$miRNA$name \n\n [1] \"hsa-mir-17\"   \"hsa-mir-505\"  \"hsa-mir-590\"  \"hsa-mir-130b\" \"hsa-mir-20a\" \n [6] \"hsa-mir-106a\" \"hsa-mir-106b\" \"hsa-mir-197\"  \"hsa-mir-186\"  \"hsa-let-7d\"  \n\n\nWhat are the variables for mRNA?\n\n\nShow the code\nselectVar(final.diablo.model, block = 'mRNA', comp = 1)$miRNA$name \n\n\nShould we go ahead with these variables? Well, maybe! We need to do another performance check of the final model.\n\nset.seed(123)\nperf.diablo = perf(final.diablo.model, validation = 'Mfold', \n                   folds = 10, nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nSo based on what we see, we are seeing similar performance using only maximum of 18 variables. However, please note that these variables are not fixed when doing cross-validation. This means that different subsets of variables might be selected in different folds of the cross-validation, leading to variability in the selected features. This variability underscores the importance of feature stability, a crucial aspect to consider when evaluating the reliability of the selected features. It is not just about how well the model performs, but also about how consistent the selected features are across different subsets of the data.\nMoving towards variable stability, it becomes essential to assess the robustness of the selected variables. Are these variables consistently selected across different folds and repetitions of cross-validation? If a variable is often selected, it indicates that its inclusion in the model is not a product of random chance or specific to a particular subset of data, enhancing our confidence in its relevance and reliability.\nIn the context of mixOmics and DIABLO, the perf() function provides insights into feature stability. By examining the stability scores, we can identify which features are consistently selected across multiple iterations of the model fitting process. A higher stability score indicates that a feature is consistently chosen across different folds and repetitions, marking it as a reliable variable that contributes significantly to the model’s predictive power.\nLet’s visualize this stability to have a clearer insight into the consistency of feature selection. We will plot the stability scores for each feature across all components and omics, giving us a comprehensive view of which features are most stable and potentially the most biologically relevant for further investigation.\n\nall_reps &lt;- perf.diablo$features$stable\n\n# get the union of all features across all repetitions for each component\nall_features_comp1 &lt;- Reduce(union, lapply(all_reps, function(x) names(x$miRNA$comp1)))\nall_features_comp2 &lt;- Reduce(union, lapply(all_reps, function(x) names(x$miRNA$comp2)))\nall_features_comp3 &lt;- Reduce(union, lapply(all_reps, function(x) names(x$miRNA$comp3)))\n\n# define function to fill in missing features with zeros\nfill_missing_features &lt;- function(stability_scores, all_features) {\n  filled_scores &lt;- rep(0, length(all_features))\n  names(filled_scores) &lt;- all_features\n  filled_scores[names(stability_scores)] &lt;- stability_scores\n  return(filled_scores)\n}\n\n# combine the stability scores for each feature across all repetitions and filling in missing features with zeros\ncombined_stability_miRNA_comp1 &lt;- Reduce('+', lapply(all_reps, function(x) \n  fill_missing_features(x$miRNA$comp1, all_features_comp1))) / length(all_reps)\n\ncombined_stability_miRNA_comp2 &lt;- Reduce('+', lapply(all_reps, function(x) \n  fill_missing_features(x$miRNA$comp2, all_features_comp2))) / length(all_reps)\n\ncombined_stability_miRNA_comp3 &lt;- Reduce('+', lapply(all_reps, function(x) \n  fill_missing_features(x$miRNA$comp3, all_features_comp3))) / length(all_reps)\n\n# plot the combined stability scores\npar(mfrow=c(1,3))\n\n# Component 1\nbarplot(combined_stability_miRNA_comp1, main=\"miRNA - Component 1\", \n        xlab=\"\", ylab=\"Combined Stability\", las=2, cex.names=0.7)\n\n# Component 2\nbarplot(combined_stability_miRNA_comp2, main=\"miRNA - Component 2\", \n        xlab=\"\", ylab=\"Combined Stability\", las=2, cex.names=0.7)\n\n# Component 3\nbarplot(combined_stability_miRNA_comp3, main=\"miRNA - Component 3\", \n        xlab=\"\", ylab=\"Combined Stability\", las=2, cex.names=0.7)\n\n\n\n\n\n\n\n\nIn the above plots each bar represents a specific feature from the omics data, and the height of the bar corresponds to the stability score of that feature. A higher bar indicates that the feature is consistently selected across different folds and repetitions of cross-validation, suggesting it is a robust and reliable feature in the context of our model.\nAs we examine these plots, we are particularly interested in features with higher stability scores. These are the features that have shown to be consistently important regardless of the specific data subset used for training, indicating their pivotal role in the model’s predictive capability. Each plot corresponds to a different component and omics data type, allowing us to dissect the contribution of each feature in a multi-faceted manner.\nTo interpret these plots effectively, focus on identifying features with the highest stability scores across multiple components. These features are not only influential in explaining the variance in the data but are also consistent in their performance, making them prime candidates for further biological investigation. Additionally, comparing the stability of features across different omics data types can provide insights into the integrative nature of the model, highlighting features that contribute to a comprehensive understanding of the underlying biological processes."
  },
  {
    "objectID": "mixomics.html#final-model-and-further-investigations",
    "href": "mixomics.html#final-model-and-further-investigations",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Final model and further investigations",
    "text": "Final model and further investigations\nWe finally have our fantastic model ready. It gives us good performance.\n\nplotIndiv(final.diablo.model, ind.names = FALSE, \n          legend = TRUE, \n          legend.position = \"top\", \n          title = 'DIABLO Sample Plots')\n\n\n\n\n\n\n\n\nVisual inspection of the scores reveals a pronounced separation between the three cancer subtypes, suggesting that our selected variables hold significant discriminatory power. We can have a look at how these variables influence the shared space:\n\nplotVar(final.diablo.model, var.names=TRUE, legend=TRUE)\n\n\n\n\n\n\n\n\nThis is Correlation Circle Plot, a visual representation that overlays the loadings (from variables) from each omic layer. In this plot, when variables cluster near each other specially closer to the poles, it indicates a strong correlation among them, even if they originate from different omic layers. Conversely, variables positioned on opposing poles suggest a strong negative correlation. Correlation Circle Plot offers insights into feature-level interactions, revealing the relationships among variables across different omic layers.\n\nNetwork visualization\nWhile integration has allowed us to construct this shared space and see the top variables, its true potential lies in understanding the the interactions among the variables. To go deeper into these interactions and uncover the underlying network of relationships, we can utilize mixOmics network visualization and the cim function, offering a comprehensive view of how these features interplay across different omics layers.\n\npar(mfrow=c(1,1))\nset.seed(123)\nmixOmics_graph &lt;- network(final.diablo.model, cutoff = 0.5,\ncolor.node = c(\"gold\", \"tomato\"),\nshape.node = c(\"rectangle\", \"circle\"),\n#color.edge = color.spectral(100),\nlty.edge = \"solid\", lwd.edge =  1, cex.node.name = 0.4,\nshow.edge.labels = T, interactive = FALSE, graph.scale = 0.3)\n\n\n\n\nThe above network is simply the results of calculating the correlation between the variables in the loadings space. We have selected to put the correlation cutoff on 0.7, resulting removing the variables that have no edges (lines).\nThe correlation between the variables in the loading space is calculated based on their loadings. Mathematically, this can be represented as:\n\\[ \\text{Corr}(X_i, Y_j) = \\frac{\\sum_{k=1}^n (X_{ik} - \\bar{X_i})(Y_{jk} - \\bar{Y_j})}{\\sqrt{\\sum_{k=1}^n (X_{ik} - \\bar{X_i})^2 \\sum_{k=1}^n (Y_{jk} - \\bar{Y_j})^2}} \\]\nwhere \\(X_i\\) and \\(Y_j\\) are the loading vectors of variables \\(i\\) and \\(j\\), \\(X_{ik}\\) and \\(Y_{jk}\\) are the individual loading coefficients at position \\(k\\), and \\(\\bar{X_i}\\) and \\(\\bar{Y_j}\\) are the means of the loading vectors.\nHowever, in the context of PLS, the loadings are normalized, meaning that their lengths are equal to 1. This simplifies the correlation calculation to the dot product of the loading vectors:\n\\[ \\text{Corr}(X_i, Y_j) = X_i \\cdot Y_j \\]\nThis dot product gives a value between -1 and 1, representing the correlation between the two variables in the latent space. A value close to 1 indicates a strong positive correlation, a value close to -1 indicates a strong negative correlation, and a value around 0 indicates no correlation.\nIn the network visualization, these correlation values are represented as edges connecting the nodes (variables). The strength and direction of the correlation determine the thickness and color of the edges, providing a visual representation of the relationships among variables across different omics data types. This aids in identifying groups of variables that are potentially functionally related and offers insights into the underlying biological processes.\nThis plot can be used to group the features together, see their interaction and use that for example to do joint pathway analysis or similar.\n\n\nClustering the relevant variables\nThe network function by mixOmics provides convenience when it comes to visualization but in order to perform a much more flexible analysis we would need to construct the network ourselves. We do that using the provide formula above but instead of calculating the correlations across data views, we are going to calculate it both within and across data sets so we can do clustering. We are also going to put a cutoff of 0.5 correlation, so everything with absolute correlation lower than 0.5 is set to zero.\n\ncomponents &lt;- 1:2\n\n# calculate loadings for the selected variables\nind_cord&lt;-mapply(function(x,y,z){cor(x[,apply(abs(z[,components]),1,sum)&gt;0],y[,components,drop=F],use = \"p\")},\n  x=final.diablo.model$X[-3],y=final.diablo.model$variates[-3],z=final.diablo.model$loadings[-3])\n\n# calculate cross-correlation matrix\nall_loadings&lt;-(rbind(ind_cord$miRNA,ind_cord$mRNA))\ncorrealtions&lt;-all_loadings%*%t(all_loadings)\n\n# set uninteresting correlation to zero\ncorrealtions[abs(correaltions) &lt; 0.5] &lt;- 0\n\nWe often need to use dissimlarity or sometime weights to perform clustering. Correlations are not a good measure to do clustering. We are going to use \\(\\sqrt{2\\times 1- r}\\) formula to transform the correlations to distances.We then use gaussian similarity function (\\(\\text{Similarity}(i, j) = \\exp\\left(-\\frac{\\text{Distance}(i, j)^2}{2\\sigma^2}\\right)\\)) to convert distances into similarities, ensuring that the similarities are positive and bounded between 0 and 1.\n\n# convert to distance\ncorrealtions &lt;-sqrt(2 * (1 - correaltions))\n\n# convert to similarities\nsigma &lt;- 1 \nsimilarity_matrix &lt;- exp(-correaltions^2 / (2 * sigma^2))\n\nGiven the similarity we can now create the graph using igraph package and perform community detection.\n\nlibrary(igraph)\n\n# create a graph from an adjacency matrix with weighted edges\ng &lt;- graph_from_adjacency_matrix(similarity_matrix, weighted = TRUE, diag = FALSE)\n\n# convert the graph to an undirected graph and combine edge attributes randomly\ng &lt;- as.undirected(g, edge.attr.comb = \"random\")\n\n# remove edges with zero (transformed) correlation\ng &lt;- delete.edges(g, which(E(g)$weight ==exp(-sqrt(2 * (1 - 0))^2 / (2 * sigma^2))))\n\n# remove isolated vertices (nodes with degree 0)\nIsolated = which(degree(g) == 0)\ng =delete.vertices(g, Isolated)\n\n# assign group labels based on node names\nV(g)$group &lt;- V(g)$name\nV(g)$group[V(g)$group %in% rownames(final.diablo.model$loadings$miRNA)] &lt;- \"miRNA\"\nV(g)$group[V(g)$group %in% rownames(final.diablo.model$loadings$mRNA)] &lt;- \"mRNA\"\n\n# define node and edge colors, sizes, and other attributes\ncolrs &lt;- c(\"gray50\", \"darkgreen\")\nrbPal &lt;- colorRampPalette(c('red', 'blue'))\nE(g)$color &lt;- rbPal(10)[as.numeric(cut(E(g)$weight, breaks = 10))]\nV(g)$color &lt;- colrs[as.factor(V(g)$group)]\nV(g)$size &lt;- 5\nV(g)$label.color &lt;- \"black\"\nV(g)$label &lt;- NA\nE(g)$width &lt;- 1\n\n# do clustering\nclusters &lt;- cluster_louvain(g)\n\n# plot the graph\nplot.igraph(g,mark.groups = clusters)\n\n\n\n\nWe constructed a graph from an adjacency matrix and then cleaned it by removing specific edges and isolated vertices. The nodes were then labeled and categorized, and attributes are assigned to both nodes and edges for better visualization.\nAt this stage, we want to do clustering. In graph terminology, this is called community detection. A community is a group of nodes that are more connected to each other than to nodes outside the group. There are many algorithms for community detection, but here we have selected to go with Louvain.\nThe detection of communities using the Louvain algorithm involves an iterative process of optimizing modularity (we will come back to this in a second!). Modularity is a measure that quantifies the quality of an assignment of nodes to communities by comparing the density of links within communities to links between communities. In short, the Louvain algorithm starts with each node assigned to its own community. It then proceeds in two phases that are repeated iteratively. In the first phase, for each node, the algorithm evaluates the gain in modularity that would result from removing the node from its current community and placing it into a neighboring community. The node is then placed in the community for which this gain is maximized, if there is a positive gain. This process is repeated for all nodes until no further improvement in modularity can be achieved.\nIn the second phase, the algorithm builds a new network whose nodes are the communities found in the first phase. Links between these new nodes are established by summing the weights of links between nodes in the corresponding communities of the original network. The weight of the links within the new communities is given by the total weight of the links between the nodes within those communities in the original network. The algorithm then returns to the first phase and repeats the process on this new network of communities, and these two phases are iteratively repeated until there is no further improvement in modularity.\nLouvain algorithm is known for its efficiency and is capable of handling large networks. It often reveals hierarchical structures within networks by breaking down larger communities into smaller, more tightly knit groups in successive iterations.\nModularity is a metric that quantifies the strength of division of a network into communities. It measures the density of links inside communities as compared to links between communities. The formula for modularity is \\(Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right) \\delta(c_i, c_j)\\), where \\(A_{ij}\\) is the element of the adjacency matrix (1 if there is a link between node \\(i\\) and node \\(j\\), and 0 otherwise), \\(k_i\\) and \\(k_j\\) are the degrees of nodes \\(i\\) and \\(j\\), \\(m\\) is the total number of links in the network, \\(c_i\\) and \\(c_j\\) are the communities of the nodes, and \\(\\delta\\) is the Kronecker delta function that is 1 if \\(c_i = c_j\\) and 0 otherwise. In simpler terms, modularity compares the number of links within communities to the number expected randomly, with higher values indicating stronger community structure. It is widely used in network science to evaluate the effectiveness of community detection algorithms, helping to identify the network’s underlying structure and to understand the organization and function of complex networks.\nNow we have a graph, a list of clusters (communities) and a way to say how good the clustering is modularity but one thing remains that is how good the modularity score actually is? One way to address this, is to compare the modularity to a set of modularity scores from randomly created graph and then say how often a random graph gives better modularity compare to our original graph. This frequency is going to give us a p-value for our modularity.\n\n# compute the modularity of the original network\nmodularity_original &lt;- modularity(clusters)\n\n# generate random networks and compute their modularity\nn_random_networks &lt;- 1000  # Number of random networks to generate\nmodularities_random &lt;- numeric(n_random_networks)\n\nfor (i in 1:n_random_networks) {\n  # generate a random network with the same size and degree distribution as the original network\n  set.seed(i)\n  g_random &lt;- rewire(g, keeping_degseq())\n\n  # compute the community structure of the random network\n  clusters_random &lt;- cluster_louvain(g_random)\n  \n  # compute the modularity of the random network\n  modularities_random[i] &lt;- modularity(clusters_random)\n}\n\nhist(modularities_random,xlim = c(min(modularities_random),max(c(modularity_original+0.07,max(modularities_random)))))\nabline(v=modularity_original,col=\"red\")\ntext(x=modularity_original,y=20,\"original modularity\")\n\n\n\n\n\n\n\n\nThe the code above, we created 1000 random graphs and calculated modularity for each. Do you think our communities is better than random?\nWe can calculate the frequency that random modularities are higher than our original modularity, giving us a p-value.\n\n# compare the modularity of the original network with that of the random networks\np_value &lt;- mean(modularities_random &gt;= modularity_original)\n\nWith a p-value of 0 we are now more confident in our community analysis. Let’s try to visualize these variables in the correlation plot we saw before:\n\n# your existing plot command\nplot(all_loadings[clusters$names,], \n     col=clusters$membership, \n     pch=as.numeric(as.factor(V(g)$group)))\n\n# add a color legend for clusters\nlegend(\"topright\",                      # Position of the legend on the plot\n       legend=unique(clusters$membership), # Unique cluster IDs\n       fill=unique(clusters$membership),   # Colors corresponding to cluster IDs\n       title=\"Membership\")               # Title of the legend\n\nlegend(\"bottomright\",                   # Position of the second legend on the plot\n       legend=levels(as.factor(V(g)$group)), # Unique group names\n       pch=unique(as.numeric(as.factor(V(g)$group))), # Point characters for each group\n       title=\"Data View\")                   # Title of the second legend\n\n\n\n\n\n\n\n\nThe clustering makes a lot of sense. The variables with similar direction of contribution have been clusters together meaning that they have similar effect on the score pattern we observed before. We can also have a look at the scaled expression pattern:\n\n# combine legends and colors\ncombined_legend &lt;- c(levels(final.diablo.model$Y), as.character(unique(clusters$membership)))\ncombined_colors &lt;- c(unique(as.numeric(final.diablo.model$Y)), unique(clusters$membership))\n\n# define the combined legend parameter\nlegend_param &lt;- list(\n  legend = combined_legend, # Combined legend for each group\n  col = combined_colors,    # Combined color vector\n  title = \"Combined Legend\",# Title of the legend\n  cex = 0.8                 # Size of the legend items\n)\n\n# call the cim() function with the combined legend parameter\ncim(cbind(final.diablo.model$X$miRNA, final.diablo.model$X$mRNA)[,clusters$names], \n    col.sideColors = clusters$membership,\n    row.sideColors = as.numeric(final.diablo.model$Y),\n    legend = legend_param)  # Add the combined legend parameter here\n\n\n\n\n\n\n\n\nWe can see a clear expression differences between the cluster 1 and 3 specially between the group Basal and LumA. From this step one can go forward with doing pathway analysis or directly interpret the results. We are going to move on to another important functionality of mixOmics\n\n\nPredicting new cases\nNow that we have the model, we can use it to predict new data (unseen by the model)\n\n# organize test data\ntest_data = list(miRNA = breast.TCGA$data.test$mirna, \n            mRNA = breast.TCGA$data.test$mrna)\n\n# predict cancer subtypes\npredicted_class_test &lt;- mixOmics:::predict.mint.splsda(final.diablo.model,test_data)\npred_classes_test &lt;- predicted_class_test$WeightedVote$centroids.dist[,3]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\n\n# BER\nprint(mean(class_error))\n## [1] 0.04444444\n\n# Overall\nprint(overall_error_rate)\n## [1] 0.05714286\n\nThis error rate is very good. So our model performs reasonably.\n\n\nWas the integration successful?\nWe do integration for different purposes. Here we used both for prediction of cancer suptypes and also for clustering the variables. In the case of the cancer suptypes prediction, we might want to ask the question if the integration is necessary or not?\nWe are going to test this by first doing separate PLSDA on each omics and compare the performance to the integrated one. So we are trying to do everything as similar as possible and check the error rates.\n\nmiRNA_plsda &lt;- splsda(training_data$miRNA, Y_training, ncomp = 5)\nperf.diablo_miRNA_plsda = perf(miRNA_plsda, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 10)\n\nmRNA_plsda &lt;- splsda(training_data$mRNA, Y_training,ncomp = 5)\nperf.diablo_mRNA_plsda = perf(mRNA_plsda, \n                              validation = 'Mfold', \n                              folds = 10,\n                              nrepeat = 10)\n\n# set grid of values for each component to test\ntest.keepX = list (mirna =seq(10, 18, 2),\n                  mrna = seq(10, 18, 2))\n                   \n# run the feature selection tuning\nset.seed(123)\ntune.miRNA_plsda = tune.splsda(training_data$miRNA, Y = Y_training, \n                               ncomp = perf.diablo_miRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"], \n                               test.keepX = test.keepX$mirna, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 1,\n                               dist = \"centroids.dist\")\n\nNote that the number of components cannot be reliably tuned with nrepeat &lt; 3 or validaion = 'loo'.\n\nset.seed(123)\ntune.mRNA_plsda = tune.splsda(training_data$mRNA, Y = Y_training, \n                              ncomp = perf.diablo_mRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"], \n                              test.keepX = test.keepX$mrna, \n                              validation = 'Mfold', \n                              folds = 10, \n                              nrepeat = 1,\n                              dist = \"centroids.dist\")\n\nNote that the number of components cannot be reliably tuned with nrepeat &lt; 3 or validaion = 'loo'.\n\nfinal_miRNA_plsda &lt;- splsda(training_data$miRNA, Y_training,\n                            ncomp = perf.diablo_miRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"],\n                            keepX = tune.miRNA_plsda$choice.keepX)\n\nperf.diablo_miRNA_plsda = perf(final_miRNA_plsda, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 10)\n\nfinal_mRNA_plsda &lt;- splsda(training_data$mRNA, Y_training,\n                           ncomp = perf.diablo_mRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"],\n                           keepX = tune.mRNA_plsda$choice.keepX)\n\nperf.diablo_mRNA_plsda = perf(final_mRNA_plsda, \n                              validation = 'Mfold', \n                              folds = 10,\n                              nrepeat = 10)\n\nprint(\"Erro rates for Integrated analysis\")\n\n[1] \"Erro rates for Integrated analysis\"\n\nperf.diablo$WeightedPredict.error.rate[\"Overall.BER\",]\n\n     comp1      comp2      comp3 \n0.33333333 0.08814815 0.07866667 \n\nprint(\"Erro rates for miRNA analysis\")\n\n[1] \"Erro rates for miRNA analysis\"\n\nperf.diablo_miRNA_plsda$error.rate$BER[,\"centroids.dist\"]\n\n    comp1     comp2 \n0.2589630 0.2152593 \n\nprint(\"Erro rates for mRNA analysis\")\n\n[1] \"Erro rates for mRNA analysis\"\n\nperf.diablo_mRNA_plsda$error.rate$BER[,\"centroids.dist\"]\n\n     comp1      comp2      comp3 \n0.09303704 0.08074074 0.08600000 \n\nprint(\"Prediction on the test set\")\n\n[1] \"Prediction on the test set\"\n\npredicted_class_test &lt;- predict(final_miRNA_plsda,test_data$miRNA)\n\n\npred_classes_test&lt;-predicted_class_test$class$centroids.dist[,final_miRNA_plsda$ncomp]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\nprint(\"miRNA\")\n\n[1] \"miRNA\"\n\nprint(\"BER:\")\n\n[1] \"BER:\"\n\nprint(mean(class_error))\n\n[1] 0.168254\n\nprint(\"Overall\")\n\n[1] \"Overall\"\n\nprint(overall_error_rate)\n\n[1] 0.1857143\n\n#######\npredicted_class_test &lt;- predict(final_mRNA_plsda,test_data$mRNA)\n\npred_classes_test&lt;-predicted_class_test$class$centroids.dist[,final_mRNA_plsda$ncomp]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\nprint(\"miRNA\")\n\n[1] \"miRNA\"\n\nprint(\"BER:\")\n\n[1] \"BER:\"\n\nprint(mean(class_error))\n\n[1] 0.03492063\n\nprint(\"Overall\")\n\n[1] \"Overall\"\n\nprint(overall_error_rate)\n\n[1] 0.04285714\n\n\nWhat do you think? Did the integration improve anything?"
  },
  {
    "objectID": "mixomics.html#integrative-regression",
    "href": "mixomics.html#integrative-regression",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Integrative regression",
    "text": "Integrative regression\nThe regression-based integrative analysis can also be performed as demonstrated above. However, unfortunately mixOmics does not provide functions to perform cross-validation and tuning on block.pls results. Most of the required measurements can be implement from scratch but we are going to skip it as it goes beyond the purpose of this document."
  },
  {
    "objectID": "mixomics-light.html",
    "href": "mixomics-light.html",
    "title": "Matrix Decomposition for Data Integration",
    "section": "",
    "text": "Set up the environment\n# list of packages to be installed\npackages &lt;- c(\"mixOmics\")\n\n# check and install missing packages\nnew_packages &lt;- packages[!(packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages, dependencies = TRUE, type = \"binary\")\n\n# load the libraries\nlibrary(mixOmics)"
  },
  {
    "objectID": "mixomics-light.html#pca-with-mixomics",
    "href": "mixomics-light.html#pca-with-mixomics",
    "title": "Matrix Decomposition for Data Integration",
    "section": "PCA with mixOmics",
    "text": "PCA with mixOmics\nThere are many implementation of PCA algorithm, e.g. prcomp() function in R. mixOmics implements a very powerful PCA method that provide us not only with standard PCA but also with extra advantages such as handling of missing value, plotting and taking into account repeated measurements.\n\n# extract miRNA data set from the training split\ndata_mirna &lt;- breast.TCGA$data.train$mirna\n\n# do PCA using mixOmics\npca_mixomics &lt;- mixOmics::pca(data_mirna, ncomp = 2, center = TRUE)\npca_mixomics &lt;- mixOmics::pca(data_mirna, ncomp = 2, center = TRUE)\n\n# plot the PCA\nmixOmics::plotIndiv(pca_mixomics, \n                    group=breast.TCGA$data.train$subtype, \n                    ind.names = F, \n                    legend = T, \n                    title = \"miRNA PCA\")\n\n\n\n\n\n\n\n\nIn the function above, we have performed a PCA with two components on miRNA data. The first principal component (PC1) captures 23% of the total variance, while the second principal component (PC2) accounts for an additional 9%. This means that together, PC1 and PC2 provide a simplified representation that encapsulates 32% of the total variability in the miRNA data.\nExamining the PCA plot, we can notice distinct patterns. The Basal cancer subtype is clustered on the right side of the plot, indicating an unique miRNA expression profile that is markedly different from the other subtypes. In contrast, the HER2 and LumA subtypes are more centered and somewhat intermingled towards the left, suggesting overlapping or similar patterns of miRNA expression. Most of the differences are represented in the PC1, so it is probably our most important factor to focus on!\nThis observed separation and overlap in the PCA plot is not just a graphical representation but is rooted in the underlying biology of these cancer subtypes. The positioning of the different groups on the PCA plot is influenced by the expression levels of various miRNAs, each contributing differently to the principal components.\nNow, as we go deeper into understanding the PCA plot, it becomes essential to explore the concept of loadings. Loadings help us to interpret the contribution of each miRNA to the principal components. They provide insights into which specific miRNAs are driving the separation between different cancer subtypes observed on the PCA plot.\nWe can go ahead and plot the loadings. We start with our most important PC, that is PC1\n\n# loadings for component 1\nmixOmics::plotLoadings(pca_mixomics, comp = 1)\n\n\n\n\n\n\n\n\nIn this bar plot, each bar represents a specific miRNA. The length of the bar corresponds to the value of the loading of that miRNA on PC1, indicating its contribution to this principal component. The miRNAs with the highest absolute contributions are at the bottom, and those with the lowest are at the top, making it easy to identify the most influential miRNAs. Both the length and direction of each bar provide crucial insights into the miRNA’s contribution to the first principal component (PC1). The length of the bar signifies the magnitude of the miRNA’s contribution. Longer bars indicate miRNAs that have a more substantial influence on the variance captured by PC1, highlighting them as key elements in distinguishing different patterns of gene expression within the dataset.\nThe direction of the bar adds another layer of interpretation. Bars extending to the right represent miRNAs that are positively correlated with PC1, indicating that as the values of these miRNAs increase, so does the score of PC1. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these miRNAs increase, the score of PC1 decreases. This directional information can be important in understanding the expression patterns of miRNAs in different breast cancer subtypes. For instance, miRNAs that are positively correlated with PC1 might be highly expressed in the Basal subtype but low in others, offering insights into the molecular distinctions between these cancer subtypes."
  },
  {
    "objectID": "mixomics-light.html#limitations-with-more-data-sets",
    "href": "mixomics-light.html#limitations-with-more-data-sets",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Limitations with more data sets",
    "text": "Limitations with more data sets\nScore plot together with loading give us powerful tool to investigate patterns in a single data set. But how about if we have multiple data sets? Can we simply go ahead and merge multiple data sets into one and do PCA on this merged data?\nWhile it might be tempting to merge multiple data sets into one and proceed with PCA, this approach has several challenges and limitations. Different data sets can have variations in terms of units, scales, and data collection methods. Simply merging them without addressing these issues can lead to misleading PCA results, where the observed variance is more a reflection of the data sets’ inconsistencies rather than underlying biological patterns. In addition, when data sets are collected at different times, locations, or under different conditions, batch effects can occur. These systematic non-biological differences can confound the PCA results, making it difficult to detect true patterns and relationships within the data. This leads us to multi-omics analysis, where techniques like Canonical Correlation Analysis (CCA) offer ways to detect correlated patterns between two or more omics data sets, and provide a more holistic view of the underlying biological processes."
  },
  {
    "objectID": "mixomics-light.html#partial-least-squares-and-block-pls",
    "href": "mixomics-light.html#partial-least-squares-and-block-pls",
    "title": "Matrix Decomposition for Data Integration",
    "section": "Partial Least Squares and block PLS",
    "text": "Partial Least Squares and block PLS\nPLS\nWe are now going to define an outcome variable and use that to perform supervised data integration. This is where Partial Least Squares (PLS) comes into play, a method that not only facilitates the integration of data from various sources but also enables the prediction of an outcome variable by identifying the relationships between observed variables and the outcomes of interest.\nPLS serves as a bridge between principal component analysis (PCA) and regression analysis. PLS is particularly useful when dealing with complex, high-dimensional, and multicollinear data, where traditional regression models may falter due to overfitting or multicollinearity issues.\nIn PLS, the predictor variables (or features such as miRNA or mRNA) and the response variables (or outcomes such as cancer subtypes) are projected to a new subspace formed by latent variables (or components). These latent variables are linear combinations of the original variables and are constructed in such a way that they maximize the covariance between the predictors and the response. This is a key distinction from PCA, which only considers the variance of the predictor variables.\nA classical version of PLS is applicable to two data sets only. For instance we could try to use miRNA data set as explanatory one and regress it onto protein data sets (outcome). We would end up with the single pairs of scores which maximize the covariance between miRNA and protein data sets. You can think about PLS as ordinary regression where both the predictor and response variables are simultaneously transformed to a new space defined by latent variables. These latent variables are constructed to maximize the covariance between the transformed predictor and response variables, ensuring that the most relevant features for prediction are captured.\nPLS-DA is an extension of PLS designed for classification tasks, where it helps differentiate between predefined classes or groups based on multivariate data. For instance, in our case we could try to use PLS-DA to predict cancer subtypes based on one of the omics such as miRNA. Note, the difference, to the PLS above, where we used miRNA data to explain protein values (regression task).\nFinally, both PLS and PLS-DA can be expanded to include multiple data sets, such as miRNA and mRNA, in block PLS. This can be use both for classification and regression."
  },
  {
    "objectID": "mixomics-light.html#diablo",
    "href": "mixomics-light.html#diablo",
    "title": "Matrix Decomposition for Data Integration",
    "section": "DIABLO",
    "text": "DIABLO\nUsing mixOmics one can perform multi block PLS using block.pls function. DIABLO is the name of the mixOmics framework that implements multi block PLS-DA (see here. Let’s try it out on miRNA and mRNA to eventually predict the cancer subtypes.\nWe start by defining the training data, based on miRNA and mRNA.\n\n# define training data\ntraining_data = list(miRNA = breast.TCGA$data.train$mirna, \n            mRNA = breast.TCGA$data.train$mrna)\n\nY_training &lt;- breast.TCGA$data.train$subtype\n\nWe need also to introduce the concept of design matrix. Here, it is a different concept than a design matrix used to capture study design, e.g. when running linear models for gene expression in packages as edgeR.\nHere, think of design as a grid where each cell’s value, ranging from 0 to 1, indicates the strength of the relationship between two corresponding data blocks. A value of 0 means no relationship, while 1 signifies a strong connection. When you are setting up your analysis, adjusting these values can help you emphasize or de-emphasize certain relationships, giving you the flexibility to focus on specific interactions that are of interest.\nChoosing the magnitude of the relationship is not straightforward. It for example can be from previous experiments or so. But generally choosing a very high value will have negative impact on the prediction ability of the model. Choosing a very low value on the other hand will cause discarding the relationship between the data blocks. Here we randomly choose to go with 0.2 to describe the strength of relationship between miRNA and RNA. In practice one might want to do Sensitivity Analysis or Cross-Validation to try to optimize this value.\n\n# define square matrix filled with 0.2\ndesign = matrix(0.2, ncol = length(training_data), nrow = length(training_data), \n                dimnames = list(names(training_data), names(training_data)))\ndiag(design) = 0 # set diagonal to 0s\n\nprint(design)\n\n      miRNA mRNA\nmiRNA   0.0  0.2\nmRNA    0.2  0.0\n\n\nWe can now proceed with running the analysis\n\nlibrary(mixOmics)\ndiablo.model = block.splsda(X = training_data, Y = Y_training, ncomp = 2, design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\nRight now we have a model that includes two components. We can go ahead and visualize the PLS scores.\n\n# plot results\nplotIndiv(diablo.model,ind.names = F,\n          legend = T, legend.position = \"top\")\n\n\n\n\n\n\n\n\nWe see three separate clusters formed in the PLS space in both OMICS. That looks great! But this can be misleading. We need to be able to systematically say how good our model’s performance is before making any interpretation. This means we need to check whethever we have fitted the data well enough or wherever we have overfitted the data.\nOverfitting occurs when a model is too complex, capturing noise in the training data and making it less effective in generalizing to new, unseen data. It essentially “memorizes” the training data, leading to excellent performance on that specific dataset but poor performance on a new data set. To mitigate overfitting and evaluate the model’s performance accurately, we employ techniques like cross-validation. Cross-validation involves partitioning the original training data set into a training set and a validation set. The model is then trained (build) on the training set and evaluated on the validation set. This process is repeated several times, with different partitions, to obtain a more generalized model performance metric. Techniques like k-fold cross-validation, where the original training set is divided into k subsets and the model is trained k times, each time using a different subset as the validation set, are commonly used. This rigorous evaluation process helps in assessing the model’s ability to generalize, ensuring that it performs well not just on the training data, but on unseen data as well. For more information see here.\nmixOmics provides us with a function (perf) to evaluate the model using cross-validation. We are going to do a 10-fold cross-validation and repeat it 10 times. This means that the entire dataset is divided into 10 equal parts, or “folds”. In each iteration, 9 folds are used for training the model, and the remaining fold is used for testing. This process is repeated 10 times, with each fold getting a chance to be the test set. The model’s performance is then averaged over all the iterations to get a more stable and reliable estimate of its performance. Repeating the entire 10-fold cross-validation process 10 times helps in reducing the variability associated with the random splitting of data and provides a more robust measure of the model’s accuracy and generalizability. Each repetition involves a different random split of the data into folds, ensuring diverse training and testing combinations and thus, a comprehensive evaluation.\n\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 2, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nAlright. This plots gives us a lot of information. The colors represent which distance measure has been used to assign groups to the test data. As we discussed we can use different distance measurement to predict the classes from the PLS continuous values. In practice these distances should do very similar but one might want to focus on centroid and mahalanobis distance more.\nThe type of the line (solid and dashed) shown error rate (ER) and balanced error rate (BER) respectively. Error Rate (ER) is the proportion of incorrect predictions made by the model over the total number of predictions, while Balanced Error Rate (BER) is the average error rates across all classes, ensuring that each class contributes equally to the overall error rate, even if the classes are imbalanced in size. In the plot, different line types allow for an easy visual comparison between these two metrics for each distance measure.\nThe x-axis represents the number of components included in the model, which is a crucial aspect of PLS-DA. By observing how the error rates change with the inclusion of additional components, one can identify the optimal number of components that minimizes the error rates, striking a balance between model complexity and predictive accuracy.\nIn evaluating these plots, one should look for the point where both ER and BER are minimized, indicating optimal model performance. Special attention should be given to the results obtained using centroid and Mahalanobis distances, as they often provide a more nuanced and reliable classification, especially in complex multiclass scenarios or when the data distribution is not straightforward. By comparing the error rates associated with different distance measures and numbers of components, one can fine-tune the PLS-DA model for optimal classification performance.\nperf functions gives a lot of other information but probably the most useful ones are:\n\nauc: Averaged AUC values over the nrepeat (if requested)\nMajorityVote.error.rate: ff more than one block, returns the error rate of the MajorityVote output\n\nHave a look at the help page of the function to know more.\nNow we know that we do actually fairly good because low error rate, meaning that the model is usable.\n\nExercise 2 (DIABLO) Can we make the above model better? Try changing the design matrix to different values, e.g. 0.8 or 0.1 and compare the results of cross-validation.\n\n\n\nShow the code\n# update design matrix\ndesign_new &lt;- design\ndesign_new[1,2] &lt;- design_new[2,1] &lt;- 0.1\n\n# re-run DIABLO\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 2, \n                            design = design_new) \n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n# Comment: \n# In this case selecting lower values of connection between mRNA and miRNA yields smaller ER and BER with 2 components\n# and selecting high values of 0.8 yields higher ER and BER with 2 components. \n\n\n\nTuning number of components\nI guess you also asked yourself why we should use two components. Maybe we can go higher? Or lower? We can try to tune number of components by simply including more components in the model and redoing the cross-validation.\n\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = 5, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\nset.seed(123)\nperf.diablo = perf(diablo.model, \n                   validation = 'Mfold', \n                   folds = 10, \n                   nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nThis is the same plot as before but now we have more component on the x-axis. What this plot tells us that the error rate sharply decreases from ~0.3 to 0.1 just by including two components instead of one. Then we have smaller decrease from component two to three. After that the improvement is very little. So shall we should use 3 components or five? This is where the trade-off between model complexity and performance comes into play. While adding more components can lead to a slight decrease in error rate, it also increases the complexity of the model, which can lead to more difficulty in interpretation etc. So we should go for the least complex model that gives us a reasonable error rate.\nmixOmics gives us an automated way of selecting the best component. We can see mixOmics choice using:\n\nprint(perf.diablo$choice.ncomp)\n\n$AveragedPredict\n            [,1]\nOverall.ER     4\nOverall.BER    4\n\n$WeightedPredict\n            [,1]\nOverall.ER     3\nOverall.BER    3\n\n$MajorityVote\n            max.dist centroids.dist mahalanobis.dist\nOverall.ER         5              3                3\nOverall.BER        4              3                3\n\n$WeightedVote\n            max.dist centroids.dist mahalanobis.dist\nOverall.ER         3              3                4\nOverall.BER        3              3                4\n\n\nI this case, we are going to use majority vote and mahalanobis.dist so we we choose three components and redo the analysis.\n\nncomps &lt;- perf.diablo$choice.ncomp$MajorityVote[\"Overall.BER\",\"centroids.dist\"]\ndiablo.model = block.splsda(X = training_data, \n                            Y = Y_training, \n                            ncomp = ncomps, \n                            design = design) \n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\n\n\nVariable selection\nWe now have a working model with a reasonably low error rate. We can use this model to extract the most important features (e.g. miRNA or mRNA) that gave us such a discriminatory pattern we saw in the score plots.\nWe should do that three times in fact. One for each components. We have one set of loading for each component and for each block of data. it is quite a lot of work to go through these loadings. In the following plots, the bars have a color corresponding the group which has the maximum (max in contrib) of average (mean in method) among all the groups.\n\nplotLoadings(diablo.model, comp=1, contrib='max', method='mean')\n\n\n\n\n\n\n\nplotLoadings(diablo.model, comp=2, contrib='max', method='mean')\n\n\n\n\n\n\n\nplotLoadings(diablo.model, comp=3, contrib='max', method='mean')\n\n\n\n\n\n\n\n\nSimilar to what we said before, the most important variables, according to the absolute value of their coefficients, are ordered from bottom to top. One can go ahead and examine top \\(n\\) variables, e.g. 10. They have the same interpretation as before: bars extending to the right represent features that are positively correlated with PC1, indicating that as the values of these features increase, so does the score of corresponding component. Conversely, bars extending to the left suggest a negative correlation, meaning as the values of these features increase, the score of the component decreases.\nIn this specific example we have a limited number of features so it might be feasible to do it manually. However, in practice most omics data sets are large and that will make it very difficult to choose a right cutoff to select the number of features per omics.\nFortunately, mixOmics gives us an automated way of selecting variables using tune.block.splsda function. The only thing we have to do is to select a grid of plausible values for each component to test. mixOmics will then do cross-validation and try to give us the most influential variables on the model and discard the rest. This means that, given a vector of possible number of variables, the function will pinpoint the optimal subset that contributes the most to our model’s predictive power, ensuring both efficiency and accuracy in our analysis.\n\n# set grid of plausible values for each component to test\ntest.keepX = list (mirna =seq(10, 18, 2),\n                  mrna = seq(10, 18, 2))\n                   \n# run the feature selection tuning\nset.seed(123)\ntune.TCGA = tune.block.splsda( training_data, Y = Y_training, ncomp = ncomps, \n                              test.keepX = test.keepX, design = design,\n                              validation = 'Mfold', folds = 10, nrepeat = 1,\n                              dist = \"centroids.dist\")\n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\n\nYou have provided a sequence of keepX of length: 5 for block mirna and 5 for block mrna.\nThis results in 25 models being fitted for each component and each nrepeat, this may take some time to run, be patient!\n\n\n\nYou can look into the 'BPPARAM' argument to speed up computation time.\n\nplot(tune.TCGA)\n\n\n\n\n\n\n\n\nThis bar plot can help us seeing the process of variable selection. In each component the x-axis is BER and y-axis is the combination of the number of selected variable in omics1 (mirna) and omics2 (mrna) which has been written as omics1_omics2 (for example 10_12 means 10 variables from mirna and 12 variables from mrna block). Again we are going to look for the least complex (fewer variables) model with lower BER. In component one the top 5 selections are in the top of the plot, the least numbers among them is 10_16, similarly we can select 10_14 for the second component and 10_18 for the last one. mixOmics also gives us these numbers in choice.keepX list.\n\nprint(tune.TCGA$choice.keepX)\n\n$miRNA\n[1] 10 10 10\n\n$mRNA\n[1] 16 14 18\n\n\nNow that we have the number of variables we can go ahead and do the final model:\n\n# set the optimised DIABLO model\nfinal.diablo.model = block.splsda(training_data, \n                                  Y = Y_training, \n                                  ncomp = ncomps,\n                                  keepX = tune.TCGA$choice.keepX, \n                                  design = design)\n\nDesign matrix has changed to include Y; each block will be\n            linked to Y.\n\n\nNow for each components we can see which variables have been selected. For example for component 1.\n\nselectVar(final.diablo.model, block = 'miRNA', comp = 1)$miRNA$name \n\n [1] \"hsa-mir-17\"   \"hsa-mir-505\"  \"hsa-mir-590\"  \"hsa-mir-130b\" \"hsa-mir-20a\" \n [6] \"hsa-mir-106a\" \"hsa-mir-106b\" \"hsa-mir-197\"  \"hsa-mir-186\"  \"hsa-let-7d\"  \n\n\nWhat are the variables for mRNA?\n\n\nShow the code\nselectVar(final.diablo.model, block = 'mRNA', comp = 1)$miRNA$name \n\n\nShould we go ahead with these variables? Well, maybe! In that case we need to do last performance check of the final model.\n\nset.seed(123)\nperf.diablo = perf(final.diablo.model, validation = 'Mfold', \n                   folds = 10, nrepeat = 10)\n\npar(mfrow=c(1,1))\nplot(perf.diablo)\n\n\n\n\n\n\n\n\nSo based on what we see, we are seeing similar performance using only maximum of 18 variables. However, please note that these variables are not fixed when doing cross-validation. This means that different subsets of variables might be selected in different folds of the cross-validation, leading to variability in the selected features. This variability underscores the importance of feature stability, a crucial aspect to consider when evaluating the reliability of the selected features. It is not just about how well the model performs, but also about how consistent the selected features are across different subsets of the data.\nMoving towards variable stability, it becomes essential to assess the robustness of the selected variables. Are these variables consistently selected across different folds and repetitions of cross-validation? If a variable is often selected, it indicates that its inclusion in the model is not a product of random chance or specific to a particular subset of data, enhancing our confidence in its relevance and reliability.\nIn the context of mixOmics and DIABLO, the perf() function provides insights into feature stability. By examining the stability scores, we can identify which features are consistently selected across multiple iterations of the model fitting process. A higher stability score indicates that a feature is consistently chosen across different folds and repetitions, marking it as a reliable variable that contributes significantly to the model’s predictive power. Check the Detailed Version of this lab, if interested how to examine the stability scors.\n\n\nFinal model\nWe finally have our fantastic model ready. It gives us good performance.\n\nplotIndiv(final.diablo.model, ind.names = FALSE, \n          legend = TRUE, \n          legend.position = \"top\", \n          title = 'DIABLO Sample Plots')\n\n\n\n\n\n\n\n\nVisual inspection of the scores reveals a pronounced separation between the three cancer subtypes, suggesting that our selected variables hold significant discriminatory power. We can have a look at how these variables influence the shared space:\n\nplotVar(final.diablo.model, var.names=TRUE, legend=TRUE)\n\n\n\n\n\n\n\n\nThis is Correlation Circle Plot, a visual representation that overlays the loadings (from variables) from each omic layer. In this plot, when variables cluster near each other specially closer to the poles, it indicates a strong correlation among them, even if they originate from different omic layers. Conversely, variables positioned on opposing poles suggest a strong negative correlation. Correlation Circle Plot offers insights into feature-level interactions, revealing the relationships among variables across different omic layers.\n\n\nNetwork visualization\nWhile integration has allowed us to construct this shared space and see the top variables, its true potential lies in understanding the the interactions among the variables. To go deeper into these interactions and uncover the underlying network of relationships, we can utilize mixOmics network visualization and the cim function, offering a comprehensive view of how these features interplay across different omics layers.\n\npar(mfrow=c(1,1))\nset.seed(123)\nmixOmics_graph &lt;- network(final.diablo.model, cutoff = 0.5,\ncolor.node = c(\"gold\", \"tomato\"),\nshape.node = c(\"rectangle\", \"circle\"),\nlty.edge = \"solid\", lwd.edge =  1, cex.node.name = 0.4,\nshow.edge.labels = T, interactive = FALSE, graph.scale = 0.3)\n\n\n\n\nThe above network is simply the results of calculating the correlation between the variables in the loadings space. We have selected to put the correlation cutoff on 0.7, resulting removing the variables that have no edges (lines).\n\n\nPredicting new cases\nNow that we have the model, we can use it to predict new data, cancer subtypes unseen so far by the model.\n\n# organize test data\ntest_data = list(miRNA = breast.TCGA$data.test$mirna, \n            mRNA = breast.TCGA$data.test$mrna)\n\n# predict cancer subtypes\npredicted_class_test &lt;- mixOmics:::predict.mint.splsda(final.diablo.model,test_data)\npred_classes_test &lt;- predicted_class_test$WeightedVote$centroids.dist[,3]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\n\n# BER\nprint(mean(class_error))\n## [1] 0.04444444\n\n# Overall\nprint(overall_error_rate)\n## [1] 0.05714286\n\nThis error rate is very good. So our model performs reasonably.\n\n\nWas the integration successful?\nIn the case of the cancer suptypes prediction, we might want to ask the question if the integration is necessary or not?\nWe are going to test this by first doing separate PLSDA on each omics and compare the performance to the integrated one. So we are trying to do everything as similar as possible and check the error rates.\n\nmiRNA_plsda &lt;- splsda(training_data$miRNA, Y_training, ncomp = 5)\nperf.diablo_miRNA_plsda = perf(miRNA_plsda, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 10)\n\nmRNA_plsda &lt;- splsda(training_data$mRNA, Y_training,ncomp = 5)\nperf.diablo_mRNA_plsda = perf(mRNA_plsda, \n                              validation = 'Mfold', \n                              folds = 10,\n                              nrepeat = 10)\n\n# set grid of values for each component to test\ntest.keepX = list (mirna =seq(10, 18, 2),\n                  mrna = seq(10, 18, 2))\n                   \n# run the feature selection tuning\nset.seed(123)\ntune.miRNA_plsda = tune.splsda(training_data$miRNA, Y = Y_training, \n                               ncomp = perf.diablo_miRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"], \n                               test.keepX = test.keepX$mirna, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 1,\n                               dist = \"centroids.dist\")\n\nNote that the number of components cannot be reliably tuned with nrepeat &lt; 3 or validaion = 'loo'.\n\nset.seed(123)\ntune.mRNA_plsda = tune.splsda(training_data$mRNA, Y = Y_training, \n                              ncomp = perf.diablo_mRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"], \n                              test.keepX = test.keepX$mrna, \n                              validation = 'Mfold', \n                              folds = 10, \n                              nrepeat = 1,\n                              dist = \"centroids.dist\")\n\nNote that the number of components cannot be reliably tuned with nrepeat &lt; 3 or validaion = 'loo'.\n\nfinal_miRNA_plsda &lt;- splsda(training_data$miRNA, Y_training,\n                            ncomp = perf.diablo_miRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"],\n                            keepX = tune.miRNA_plsda$choice.keepX)\n\nperf.diablo_miRNA_plsda = perf(final_miRNA_plsda, \n                               validation = 'Mfold', \n                               folds = 10, \n                               nrepeat = 10)\n\nfinal_mRNA_plsda &lt;- splsda(training_data$mRNA, Y_training,\n                           ncomp = perf.diablo_mRNA_plsda$choice.ncomp[\"BER\",\"centroids.dist\"],\n                           keepX = tune.mRNA_plsda$choice.keepX)\n\nperf.diablo_mRNA_plsda = perf(final_mRNA_plsda, \n                              validation = 'Mfold', \n                              folds = 10,\n                              nrepeat = 10)\n\nprint(\"Erro rates for Integrated analysis\")\n\n[1] \"Erro rates for Integrated analysis\"\n\nperf.diablo$WeightedPredict.error.rate[\"Overall.BER\",]\n\n     comp1      comp2      comp3 \n0.33333333 0.08814815 0.07866667 \n\nprint(\"Erro rates for miRNA analysis\")\n\n[1] \"Erro rates for miRNA analysis\"\n\nperf.diablo_miRNA_plsda$error.rate$BER[,\"centroids.dist\"]\n\n    comp1     comp2 \n0.2589630 0.2152593 \n\nprint(\"Erro rates for mRNA analysis\")\n\n[1] \"Erro rates for mRNA analysis\"\n\nperf.diablo_mRNA_plsda$error.rate$BER[,\"centroids.dist\"]\n\n     comp1      comp2      comp3 \n0.09303704 0.08074074 0.08600000 \n\nprint(\"Prediction on the test set\")\n\n[1] \"Prediction on the test set\"\n\npredicted_class_test &lt;- predict(final_miRNA_plsda,test_data$miRNA)\n\n\npred_classes_test&lt;-predicted_class_test$class$centroids.dist[,final_miRNA_plsda$ncomp]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\nprint(\"miRNA\")\n\n[1] \"miRNA\"\n\nprint(\"BER:\")\n\n[1] \"BER:\"\n\nprint(mean(class_error))\n\n[1] 0.168254\n\nprint(\"Overall\")\n\n[1] \"Overall\"\n\nprint(overall_error_rate)\n\n[1] 0.1857143\n\n#######\npredicted_class_test &lt;- predict(final_mRNA_plsda,test_data$mRNA)\n\npred_classes_test&lt;-predicted_class_test$class$centroids.dist[,final_mRNA_plsda$ncomp]\nreadl_classes_test &lt;- breast.TCGA$data.test$subtype\n\n# create a confusion matrix\nconf_matrix &lt;- table(readl_classes_test, pred_classes_test)\n\n# calculate the class error rates\nclass_error &lt;- 1 - diag(conf_matrix) / rowSums(conf_matrix)\n\n# calculate the overall error rate\noverall_error_rate &lt;- (sum(conf_matrix) - sum(diag(conf_matrix))) / sum(conf_matrix)\nprint(\"miRNA\")\n\n[1] \"miRNA\"\n\nprint(\"BER:\")\n\n[1] \"BER:\"\n\nprint(mean(class_error))\n\n[1] 0.03492063\n\nprint(\"Overall\")\n\n[1] \"Overall\"\n\nprint(overall_error_rate)\n\n[1] 0.04285714\n\n\nWhat do you think? Did the integration improve anything?"
  }
]